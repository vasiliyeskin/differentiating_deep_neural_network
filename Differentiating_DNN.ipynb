{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNO5PIvUR4NheTkvgn77Y1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "ec1c3957-8f89-4063-a17f-a8d1b9eb9f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3",
        "outputId": "ad6b9b2d-78a5-463d-98a4-46f7794a8fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = 'vasiliyeskin'\n",
        "# user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "repo_name = 'differentiating_deep_neural_network'\n",
        "destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "### run first time if repo is absence\n",
        "# cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "### run next times\n",
        "cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "print(cmd_string)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Password: ··········\n",
            "git -C '/content/gdrive/My Drive/differentiating_deep_neural_network' pull\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "bd2be61c-62be-4346-b648-541463a7a4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "b86e8c36-90e9-4a29-bd3b-19f3202cbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "de7be5c0-85cf-4d8e-f69f-e1a17a88d28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "edfd22ac-f1a4-4347-a834-9197f9b0c733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "f048c587-b8f2-41c3-f1dd-01298c2ca31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "80a336fd-7b20-4569-d4c8-c235f0a0f19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "2ef6e750-6d60-494a-beee-d2afef3589ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "b5ff5dc0-2de3-465f-a039-834bc0fe362d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1050\n",
            "Epoch 1 Batch 100 Loss 0.5480\n",
            "Epoch 1 Loss 0.6971\n",
            "Time taken for 1 epoch 28.143463850021362 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5330\n",
            "Epoch 2 Batch 100 Loss 0.5850\n",
            "Epoch 2 Loss 0.5862\n",
            "Time taken for 1 epoch 11.381104230880737 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4848\n",
            "Epoch 3 Batch 100 Loss 0.5116\n",
            "Epoch 3 Loss 0.5227\n",
            "Time taken for 1 epoch 11.019082307815552 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5318\n",
            "Epoch 4 Batch 100 Loss 0.3834\n",
            "Epoch 4 Loss 0.4050\n",
            "Time taken for 1 epoch 11.500761270523071 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2147\n",
            "Epoch 5 Batch 100 Loss 0.2067\n",
            "Epoch 5 Loss 0.2262\n",
            "Time taken for 1 epoch 11.20753526687622 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0541\n",
            "Epoch 6 Batch 100 Loss 0.0787\n",
            "Epoch 6 Loss 0.0783\n",
            "Time taken for 1 epoch 11.695879936218262 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0140\n",
            "Epoch 7 Batch 100 Loss 0.0380\n",
            "Epoch 7 Loss 0.0340\n",
            "Time taken for 1 epoch 11.421447992324829 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0095\n",
            "Epoch 8 Batch 100 Loss 0.0089\n",
            "Epoch 8 Loss 0.0557\n",
            "Time taken for 1 epoch 11.892610788345337 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0048\n",
            "Epoch 9 Batch 100 Loss 0.0088\n",
            "Epoch 9 Loss 0.0055\n",
            "Time taken for 1 epoch 11.661799192428589 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0030\n",
            "Epoch 10 Batch 100 Loss 0.1531\n",
            "Epoch 10 Loss 0.2473\n",
            "Time taken for 1 epoch 12.140012264251709 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "25a85755-1ff8-4991-c0d8-5826872286ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4c03023400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "4d523569-a2c1-4fd6-b7d3-d6f5f3a3d48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c c b b b a \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "dc1851c2-3f76-485e-fc0d-f55c1eb2d684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c c b a \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj"
      },
      "source": [
        "# install torchtext with BLUE module\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "    I = 0\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "        pred_trg, _ = evaluate_of_tensor(tf.reshape(src,[1, src.shape[0]]), attention_plot)\n",
        "        # print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trg_seq = tensor_to_sequence(trg)\n",
        "        trgs.append(trg_seq)\n",
        "        # print('Pred: {0}'.format(pred_trg))\n",
        "        # print('Trg:  {0}'.format(trg_seq))\n",
        "        I += 1\n",
        "        if I > 10:\n",
        "          break\n",
        "\n",
        "    print([pred_trgs])\n",
        "    print(trgs)\n",
        "    # return bleu_score(pred_trgs, trgs)\n",
        "    return sentence_bleu(pred_trgs, trgs)\n",
        "    # reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
        "    # candidate = ['the', 'quick', '23', '23', '23', 'over', 'the', 'lazy', 'dog']\n",
        "    # return sentence_bleu(reference, candidate)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "68cdfcb1-5e68-49c0-d210-cb46143112a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['a a c b b c b a a a a b ', 'a c b ', 'c b a b a b ', 'b c c b c b b b ', 'b a a c a c a c a c a b a ', 'b a c a a a c c c c ', 'b a a b a a a a b a c b b ', 'c a b b b c c c a b b c ', 'a b a ', 'a c c c ', 'b a b a a c b b c a b a ']]\n",
            "['a b a c b b c b a a a a b <eos>', 'a c b <eos>', 'c b a b a b <eos>', 'b c c b c b b b <eos>', 'b a b a c a c a c a b a <eos>', 'b a c a a a c c c c <eos>', 'b a b a a a b a c b b <eos>', 'c a b b b b c c c a b b c <eos>', 'a b a <eos>', 'a c c c <eos>', 'b a b a a c b b c a b a <eos>']\n",
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nujd6bFSifds"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}