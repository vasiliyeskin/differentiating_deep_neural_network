{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNo0cQgK9q2Yxr6YtJybf5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv\n",
        "import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeK-BFaZm2UH"
      },
      "source": [
        "teacher_forcing_ratio = 0.75"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    # forward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # backward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # self.gru = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,\n",
        "    #                      input_shape=(embedding_dim, self.dec_units))\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "6c6ac27e-a270-4017-c146-bc95b0d67bd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3"
      },
      "source": [
        "# import os\n",
        "# from getpass import getpass\n",
        "# import urllib\n",
        "\n",
        "# user = 'vasiliyeskin'\n",
        "# # user = input('User name: ')\n",
        "# password = getpass('Password: ')\n",
        "# password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# # repo_name = input('Repo name: ')\n",
        "# repo_name = 'differentiating_deep_neural_network'\n",
        "# destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "# ### run first time if repo is absence\n",
        "# # cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "# ### run next times\n",
        "# cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "# print(cmd_string)\n",
        "# os.system(cmd_string)\n",
        "# cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "c4f2e545-6f4d-4c80-b230-0ee410df6b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "a912e67f-d40e-4ff1-ab3f-82fd896e7d8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "915956e1-7ec2-47f3-a64f-321b74566c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "47c44de2-f537-417c-8fc7-5021dfb538b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "3b8bcf99-ad67-4a6a-cc2e-d53842286556",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "ee2fb569-2514-4a2c-ce12-7a1993dff079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "7e41883b-5a54-4e14-d0ac-2c63de5be5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden, teacher_forcing_ratio = 0.5):\n",
        "  loss = 0\n",
        "\n",
        "  #decide if we are going to use teacher forcing or not\n",
        "  teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      pred = tf.argmax(predictions[:, 0])\n",
        "\n",
        "      #if teacher forcing, use actual next token as next input\n",
        "      #if not, use predicted token\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1) if teacher_force else tf.expand_dims(pred,1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3pXQDBUvjrt"
      },
      "source": [
        "@tf.function\n",
        "def validation_step(src, trg, enc_hidden, size):\n",
        "  loss = 0\n",
        "\n",
        "  enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  dec_src = tf.expand_dims([src_lang.word_index['<sos>']] * size, 1)\n",
        "\n",
        "  for t in range(1, max_length_src):\n",
        "    # passing enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "    loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "    dec_src = tf.expand_dims(predictions, 1)\n",
        "    \n",
        "    print(dec_src)\n",
        "    if  dec_src == trg_lang.word_index['<sos>']:\n",
        "      break\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "  return batch_loss"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "9cbe5230-6e5c-496b-f4a7-b0a5d0eec275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "  # val_loss = validation_step(src, trg, enc_hidden, 64)\n",
        "  # print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1, val_loss))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1218\n",
            "Epoch 1 Batch 100 Loss 0.6105\n",
            "Epoch 1 Loss 0.7016\n",
            "Time taken for 1 epoch 20.25704050064087 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6014\n",
            "Epoch 2 Batch 100 Loss 0.5869\n",
            "Epoch 2 Loss 0.5746\n",
            "Time taken for 1 epoch 11.165014505386353 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5090\n",
            "Epoch 3 Batch 100 Loss 0.5498\n",
            "Epoch 3 Loss 0.5201\n",
            "Time taken for 1 epoch 10.843684196472168 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3969\n",
            "Epoch 4 Batch 100 Loss 0.4951\n",
            "Epoch 4 Loss 0.3891\n",
            "Time taken for 1 epoch 11.1401948928833 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4305\n",
            "Epoch 5 Batch 100 Loss 0.0764\n",
            "Epoch 5 Loss 0.2066\n",
            "Time taken for 1 epoch 10.923363208770752 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0470\n",
            "Epoch 6 Batch 100 Loss 0.1307\n",
            "Epoch 6 Loss 0.2076\n",
            "Time taken for 1 epoch 11.193909168243408 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0185\n",
            "Epoch 7 Batch 100 Loss 0.0525\n",
            "Epoch 7 Loss 0.1873\n",
            "Time taken for 1 epoch 10.99651837348938 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1409\n",
            "Epoch 8 Batch 100 Loss 0.0028\n",
            "Epoch 8 Loss 0.0225\n",
            "Time taken for 1 epoch 11.294432640075684 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0046\n",
            "Epoch 9 Batch 100 Loss 0.0019\n",
            "Epoch 9 Loss 0.0056\n",
            "Time taken for 1 epoch 11.052996397018433 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0011\n",
            "Epoch 10 Batch 100 Loss 0.0014\n",
            "Epoch 10 Loss 0.0052\n",
            "Time taken for 1 epoch 11.282770872116089 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id]\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "c06bd220-3e37-44ee-ea13-a77a5fd350a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3364d76390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "9cf9ee1e-1e97-4fd0-e0d9-76d6999557a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c b b b a <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAJECAYAAAB9z6IbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfZElEQVR4nO3dfdSkd13f8c83u8kuSQhqCA8JYBUJEDEQs/JQ5OEQBFSkiiA9pEBbdREFDYmWokSgFhJAgSAtEivVBXwELEWPUghokCZQRMBIUKJVCIHwYCCLJgvsfvvHTGDPdgO7ycxc92/u1+ucObnnmuve+f6yu7nfua5rZqq7AwDAmI6YegAAAG46MQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAcBNU1TdU1c9X1e2mnEPMLUlVHVlVD6yq7VPPAgAsxZlJnp3k3045hJhbnu9P8vYk/3rqQQCApXhiknfN/zkZMbc8T0ry8Uxc6wDA4lXVKUlOTfL4JHeqqu+YahYxtwRVdZsk35VZqd+/qr5x4pEAgMV6UpI3d/ffJ/n9THjwRswtx5lJ3tfdb8vsVOukh18BgMWpqiMy+1m/a77pNUkeV1VHTjGPmFuOJyV59fzr1yR5woSzAACL9dAkRyf5n/P7b0nyhSSPnGIYMbdgVXVqkrsn+a35ptcnObGq7jfdVADAAj0xyeu6+wtJ0t37Mvu5/6QphhFzi/ekJH/c3Z9Jku7+pyT/I14IAQDDq6pbJvmBfOUU6w1ek+R7qurWq55JzC1QVW3J7FUtrz7godckeWxVHbX6qQCABToiyXd395/tv7G7/yLJQ5LsXfVA1d2rfs61VVW3T/KjSc6/4dDrfPsRSX42ya7u/shU8wEA60fMAQAchqo6IUm6+1Pz+9+W5HFJ/qq7f+urfe8yOM26ZFV1i6p6qPeaA4C18btJvi9J5tfIXZzZdXS/UlXnrHoYMbdgVfXrVfXj86+PSvLuJP8ryV9X1XdPOhwAsAinJrl0/vVjklzR3d+a2atcn7zqYcTc4j08X/kNflSSWya5XZLnzG8AwNhukeTz868fmq+839x7k9xx1cOIucX7+iSfnH/9iCSv7+5PJvntJKdMNhUAsCgfTvLoqrpjkodldgYuSW6b5LOrHkbMLd4nktxj/jYlD0/y1vn2Y5N8cbKpAIBFeW6SFyT5+ySXdve75tsfnuQvVj3M1lU/4SbwqiS/k+SqzN5r5qL59vsk+dBUQwEAi9Hdb6iqOyU5Mcn793vorZl98tNKeWuSJaiqH0xypyS/191Xzrc9Kclnu/uNkw4HACxMVR2bpOef+DTNDGIOAODwVNVPJHlGkpPmm65M8oLu/q+rnsVp1iWoqlOT/HRmL3joJB9M8qLuvmzSwQCAm62qfjbJM5P8YpIbPtbrAUnOr6rjuvv8lc7jyNxiVdWjkrwhyTvyld/g75zfHt3db5pqNgDg5quqjyR5xoGf9lBVZyZ5fnev9IMCxNyCVdUHkvx+dz/7gO3/Kcm/6u57TjMZALAIVXV9knt09xUHbL9Lkr/s7u2rnMdbkyzeyUlefZDtr05y1xXPAgAs3t8kefxBtj8+yV+veBbXzC3BJ5OcnuSKA7afnuTq1Y8DACzYc5L8blU9MMk759vun+RBSR676mHE3OL9apJXVtW3JPnf8233z+wFES+abCoAYCHm7zN3nyRPT/LI+ebLk9y7u1f+psGumVuwqqokZyU5J7M3E0xmbyD8oiQva//CAYAFEnNLVFW3TJLu3j31LADA4lTVbZM8Ick3J/n57v50Vd0/yVXd/X9XOYsXQCxYVR1RVUckX464Y6rqR6rqX048GgCwAFV1emYvdDgzyY8kOW7+0Hcled6q5xFzi/eHSZ6WfPkjPt6T2SnWP62qJ045GACwEL+Y5ILuPi3Jnv22vzmz6+RXSswt3o4kb5t//egk1ya5TZIfzexFEADA2E5P8hsH2f7xJLdd8SxibgmOTfLZ+dcPy+wNhL+YWeDdebKpAIBFuS7J1x9k+90ye4uylRJzi/eRJPevqmOSPDzJW+bbvyHJP082FQCwKG9M8uyq2ja/31X1L5K8IMnrVz2MmFu8F2f2aQ9XJvlYkovn2x+Y5C+nGgoAWJifzuwgzaeSHJ3ZZ7FfkeRzSZ616mG8NckSzF/lcqckb+nuz8+3fW+Sz3b3O7/qNwMAQ6iqhyT59swOjr23u986yRxibnGq6lZJTu3udxzksfsn+WB3X7P6yQCARdiIP+udZl2sfUn+aP6b+WVVdc/MXgCxZZKpAIBF2XA/63026wJ19+6qemOSJ+YrH7ybzN4h+s3d/elpJrtpqmprkntndsr4qP0f6+5dkwx1M83f+y83nP4embUArN5G/FnvyNzi7Ury2Ko6Kpl9IkSSxyf59SmHOlxVdbfMPjT44iSvTfLfMlvDryZ5+XST3TRVdVZVfSSzi1M/V1Ufraqnzz9LdyjWsrFU1fOq6scOsv3HquoXppjpplqXtazLOhJr2cA21M96Mbd4b8ns/WceOb9/RmZHtd402UQ3zUuT/HmSW2X2lip3z+wNkd+X5AcnnOuwVdULkzwnySsz+6iV70ryK0l+PrOXkQ/DWjakJyT5i4Ns//PM/s99JOuylnVZR2ItG9WG+lnvBRBLUFUvSHLX7v7+qtqVZHd3/8TUcx2OqvpMkgd192VV9bkk9+7uv66qByX55e4+deIRD1lV/WOSnd39ugO2PybJK7v7+GkmO3zWsvFU1fVJTunuvztg+zdndiH09mkmO3zrspZ1WUdiLRvZRvpZ78jccuxK8oiqulOSH8jBP/Jjo6t85U2OP5XkpPnXVyb5lkkmunk+cCPbRvw7YC0by0eSPOAg2x+Y2d+XkazLWtZlHYm1bGQb5me9F0AsQXf/VVVdltm1Zld297unnukmuCzJPZP8XZJ3J3lGVe3N7DNmr5hysJtgV5KfSPJTB2x/SmZv8DwSa9l4XpnkJfNrZ274XOYzkpyXsU4XJ+uzlnVZR2ItG9ZG+lkv5pZnV2bXnf3c1IPcRM9Lcsz862cl+cMkb0/y6SQ/NNVQh6qqXrbf3a1J/k1VPTzJpfNt90lyYmZ/CTc0a9nYuvuXqurWSV6Wr7zq+wtJLujuF0432eFbl7WsyzoSaxnAhvhZ75q5Jamqb0jytMyu/fnE1PMswnxN1/QAf2iq6u2HuGt390OWOszNZC1jmH8e8ynzu5eP/DYr67KWdVlHYi0b1Ub5WS/mAAAGNtJFxgAAHEDMAQAMTMwtWVXtnHqGRViXdSTWshGtyzoSa9mo1mUt67KOxFoWScwt37r8YV2XdSTWshGtyzoSa9mo1mUt67KOxFoWRswBAAxs076a9aja1tu//DZqy/PF7MmR2bb051m2dVlHYi0b0bqsI7GWjWpd1rKKdZx86j9/7Z0W4FOf2ZsTjt+y1Of4mw8cvdRf/war+H3ZnWs+3d0nHOyxTfumwdtzTO5TZ0w9BgBsKG9+8/umHmFhHn7ivaYeYWHe2q/7hxt7zGlWAICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAga1FzNXMOVX14araU1VXVtV5U88FALBsW6ceYEGen+QpSc5OcnGSE5KcNulEAAArMHzMVdWxSZ6e5KzuftV88xVJLjnIvjuT7EyS7Tl6ZTMCACzLOpxmPSXJtiQXfa0du/vC7t7R3TuOzLblTwYAsGTrEHMAAJvWOsTc5Un2JDlj6kEAAFZt+Gvmunt3VV2Q5Lyq2pPZCyCOT3J6d79i2ukAAJZr+Jibe2aSa5Kcm+QOSa5OsmvSiQAAVmAtYq679yU5f34DANg01uGaOQCATUvMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxs69QDABvcEVumnmBh6sj1+E9eHXXU1CMsTN3uhKlHWJjd37Yea/mmP9ox9QgLc3LeM/UIK+HIHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDA1iLmauacqvpwVe2pqiur6ryp5wIAWLatUw+wIM9P8pQkZye5OMkJSU6bdCIAgBUYPuaq6tgkT09yVne/ar75iiSXHGTfnUl2Jsn2HL2yGQEAlmUdTrOekmRbkou+1o7dfWF37+juHUdm2/InAwBYsnWIOQCATWsdYu7yJHuSnDH1IAAAqzb8NXPdvbuqLkhyXlXtyewFEMcnOb27XzHtdAAAyzV8zM09M8k1Sc5NcockVyfZNelEAAArsBYx1937kpw/vwEAbBrrcM0cAMCmJeYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABrZ16gEmU5Xatm3qKRZj796pJ1iMWp//t6jta/JnK0ltWZ/fl7X5M3ZETT3Bwuw9/tipR1iYa+6yZeoRFuJbdn1h6hE4TGvyXzYAgM1JzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADGwtYq5mzqmqD1fVnqq6sqrOm3ouAIBl2zr1AAvy/CRPSXJ2kouTnJDktEknAgBYgeFjrqqOTfL0JGd196vmm69IcslB9t2ZZGeSbM/RK5sRAGBZ1uE06ylJtiW56Gvt2N0XdveO7t5xZG1f/mQAAEu2DjEHALBprUPMXZ5kT5Izph4EAGDVhr9mrrt3V9UFSc6rqj2ZvQDi+CSnd/crpp0OAGC5ho+5uWcmuSbJuUnukOTqJLsmnQgAYAXWIua6e1+S8+c3AIBNYx2umQMA2LTEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwLZOPcBUqiq1ZcvUYyxETz3AglTV1CMsTB115NQjLEwduT5ryZqspW+xbeoRFuZjDz526hEW5vaXXDf1CAtxxDs+MPUIHCZH5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGthYxVzPnVNWHq2pPVV1ZVedNPRcAwLJtnXqABXl+kqckOTvJxUlOSHLapBMBAKzA8DFXVccmeXqSs7r7VfPNVyS5ZLqpAABWY/iYS3JKkm1JLvpaO1bVziQ7k2R7HbPksQAAlm8trpk7VN19YXfv6O4dR9X2qccBALjZ1iHmLk+yJ8kZUw8CALBqw59m7e7dVXVBkvOqak9mL4A4Psnp3f2KaacDAFiu4WNu7plJrklybpI7JLk6ya5JJwIAWIG1iLnu3pfk/PkNAGDTWIdr5gAANi0xBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADCwrVMPwM1XVVOPsBhbtkw9weJsXZ+/Wn2rW049wsJ87p63nnqEhdh75NQTLM4dLto99QgL03/+walHWIx9e6eegMPkyBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwNYi5mrmnKr6cFXtqaorq+q8qecCAFi2rVMPsCDPT/KUJGcnuTjJCUlOm3QiAIAVGD7mqurYJE9PclZ3v2q++Yoklxxk351JdibJ9jpmZTMCACzLOpxmPSXJtiQXfa0du/vC7t7R3TuOqu3LnwwAYMnWIeYAADatdYi5y5PsSXLG1IMAAKza8NfMdffuqrogyXlVtSezF0Acn+T07n7FtNMBACzX8DE398wk1yQ5N8kdklydZNekEwEArMBaxFx370ty/vwGALBprMM1cwAAm5aYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABjY1qkHmEwltXU9lt979049wkLUli1Tj7AwfftbTz3CwvztD91q6hEW5nbvXo+/K8f9wWVTj7Aw+667buoRFqd76gnYpByZAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABjY8DFXVX9SVS+feg4AgCkMH3MAAJuZmAMAGNi6xNzWqrqgqq6Z315UVeuyNgCAG7UuwXNmZmu5X5InJ9mZ5KxJJwIAWIGtUw+wIB9P8pPd3Uk+VFUnJzk7yYv336mqdmYWetlex6x8SACARVuXI3OXzkPuBpckOamqjtt/p+6+sLt3dPeOo47YvtoJAQCWYF1iDgBgU1qXmLtPVdV+9++b5KruvnaqgQAAVmFdYu7EJC+tqrtW1WOS/EySl0w8EwDA0q3LCyBem2RLkncl6SS/FjEHAGwCw8dcdz94v7tPnWoOAIAprMtpVgCATUnMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxMzAEADEzMAQAMTMwBAAxs69QDTKX3dfZdd/3UYyxEf+mLU4+wEEcce+zUIyzMZ+51q6lHWJgtd/n81CMszC2e9cGpR1iIffv2Tj0CsIE4MgcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMDAxBwAwMDEHADAwMQcAMLDhY66q/qSqXj71HAAAUxg+5gAANjMxBwAwsHWJua1VdUFVXTO/vaiq1mVtAAA3al2C58zM1nK/JE9OsjPJWZNOBACwAlunHmBBPp7kJ7u7k3yoqk5OcnaSF087FgDAcq3LkblL5yF3g0uSnFRVx+2/U1XtrKr3VNV7vtjXr3ZCAIAlWJeYOyTdfWF37+juHUfW9qnHAQC42dYl5u5TVbXf/fsmuaq7r51qIACAVViXmDsxyUur6q5V9ZgkP5PkJRPPBACwdOvyAojXJtmS5F1JOsmvRcwBAJvA8DHX3Q/e7+5Tp5oDAGAK63KaFQBgUxJzAAADE3MAAAMTcwAAAxNzAAADE3MAAAMTcwAAAxNzAAADE3MAAAMTcwAAAxNzAAADE3MAAAMTcwAAAxNzAAADE3MAAAMTcwAAAxNzAAADE3MAAAMTcwAAA9s69QBTqSS1ZT1adsvxt5l6hIW47tQ7Tj3Cwlz//Z+deoSF+aYfvnrqERZm7769U48AsHDrUTMAAJuUmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABjY8DFXVX9SVS+feg4AgCkMH3MAAJuZmAMAGNi6xNzWqrqgqq6Z315UVeuyNgCAG7UuwXNmZmu5X5InJ9mZ5KxJJwIAWIGtUw+wIB9P8pPd3Uk+VFUnJzk7yYv336mqdmYWetlex6x8SACARVuXI3OXzkPuBpckOamqjtt/p+6+sLt3dPeOo7JttRMCACzBusQcAMCmtC4xd5+qqv3u3zfJVd197VQDAQCswrrE3IlJXlpVd62qxyT5mSQvmXgmAIClW5cXQLw2yZYk70rSSX4tYg4A2ASGj7nufvB+d5861RwAAFNYl9OsAACbkpgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGNjWqQeYSndn3549U4+xEB/593eeeoSF+Oc7fmnqERbm7k/82NQjLMzea6+degQAvgpH5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGNnzMVdUjquodVXVNVf1jVb25qu4+9VwAAKswfMwlOSbJS5PcO8mDk3wuyZuq6qgphwIAWIWtUw9wc3X36/e/X1X/Lsm1mcXdnx3w2M4kO5Nke45e1YgAAEsz/JG5qrpzVf1mVf1tVV2b5OrM1nWnA/ft7gu7e0d37zgy21Y+KwDAog1/ZC7JHyS5MsmTk3wsyZeSfDCJ06wAwNobOuaq6vgkd0vy49399vm2b8/g6wIAOFSjR881ST6d5Eer6qNJTkryosyOzgEArL2hr5nr7n1JHpfk1CSXJfkvSc5NsmfKuQAAVmX0I3Pp7rcluccBm4+dYhYAgFUb+sgcAMBmJ+YAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABibmAAAGJuYAAAYm5gAABrZ16gG4+V72w6+ceoSFePEZ3zv1CAvzpd27px4BgE3CkTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIGJOQCAgYk5AICBiTkAgIEtJOaq6riq+rpF/FqH8FxfV1XHreK5AAA2upscc1W1paoeXlW/meQTSe45336rqrqwqj5ZVbur6k+rascB3/voqvrLqtpTVR+tqp+rqjrg8Q9U1XVV9Y/zX+O284fvmeQTVfXa+fNvualrAAAY3WHHXFV9a1W9MMlHk/xOkn9K8ogkF8+D7A+TnJTkkUlOS3JxkrdV1e3n3396kt9L8oYk35bkPyZ5ZpKnzh+/XZLfTvIbSe6e5IFJXr3fCBfPn++6+fN/pKpeWFXferhrAQAY3dZD2amqjk9yZpInZRZgf5zkp5K8qbuv32+/hyS5V5ITuvu6+eZzq+r7kjwhyQuTnJ3kT7v72fPH/6aq7pLkGUl+OcmJSY5M8rru/of5Ppfd8Bzd3ZkF3cVV9dQkj0ryxCTvq6r3J9mV5LXd/ZmDrGNnkp1Jsj1HH8rSAQA2tEM9Mve0JBckuT7Jyd39qO7+vf1Dbu70JEcn+VRVff6GW5J7JLnzfJ+7J3nnAd/3Z0lOml8L9/4kb01yWVW9vqqeUlUnHGyo7r6+u3+3ux+Z5OQkX5zP+bQb2f/C7t7R3TuOzLZDXDoAwMZ1SEfmklyYWSg9MbPI+v3MTn1e1N1799vviCRXJ3nAQX6Naw/hebq791bVw5LcN8nDkvxwkvOq6kHd/f79d55fL/fQzI76/UBmp36fleS/H+K6AACGdkhH5rr7qu5+XnffNbN4+nxm17VdWVW/VFX3mu/63iS3TbKvu6844PbJ+T6XJ7n/AU/xnUmu7O7d8+fr7r6ku5+b5DuSXJXkcTfsXFWnVdWLk1yZ5LeS7E5yRnffbT7nVYf/rwIAYDyHemTuy7r70iSXVtVZSb4vs+vo/s/8erm3ZnYK9Y1V9R+SfCjJ7TJ7wcJbu/sdSX5pvv9zkvxmZrF2TpKfTZKqum9mwfjmzI7ynZbkjkk+OH/8AUneluSPMjud+qbu3nNTFg8AMLrDjrkbzAPqdUleV1W3SbK3u7uqvifJf07yq0luk1mQvTOzFyaku99bVY9N8tzMAu7qJOcnefn8l/5cZkfunpbk6zI7dfoL3f2a+eMfTHLSfkf6AAA2rZscc/vbP6zmp0p/an67sf3fkNlbkxzsscuTfPdX+d7/71WqAACblY/zAgAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGJiYAwAYmJgDABiYmAMAGFh199QzTKKqPpXkH1bwVLdO8ukVPM+yrcs6EmvZiNZlHYm1bFTrspZ1WUdiLYfrG7v7hIM9sGljblWq6j3dvWPqOW6udVlHYi0b0bqsI7GWjWpd1rIu60isZZGcZgUAGJiYAwAYmJhbvgunHmBB1mUdibVsROuyjsRaNqp1Wcu6rCOxloVxzRwAwMAcmQMAGJiYAwAYmJgDABiYmAMAGJiYAwAY2P8DHRShUylhdDAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "036ae9da-c945-406d-b786-a9a50377c3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c b a <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj"
      },
      "source": [
        "# install torchtext with BLUE module\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NKpT4w0Dbu"
      },
      "source": [
        "def tensor_to_sequence(input):\n",
        "  result = ''\n",
        "  for id in range(1, input.shape[0]):\n",
        "    in_id = input[id].numpy()\n",
        "    result += trg_lang.index_word[in_id]\n",
        "\n",
        "    if trg_lang.index_word[in_id] == '<eos>':\n",
        "      return result\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "  return result[:-1]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "    I = 0\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data.shuffle(BATCH_SIZE)):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "\n",
        "        #cut off <eos> token\n",
        "        src_tensor = tf.reshape(src,[1, src.shape[0]])\n",
        "        #cut off <eos> token\n",
        "        # src_tensor = src_tensor[:-2]\n",
        "        # print(src_tensor)\n",
        "        pred_trg, _ = evaluate_of_tensor(src_tensor, attention_plot)\n",
        "        # print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        # pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trg_seq = tensor_to_sequence(trg)\n",
        "        trgs.append(trg_seq)\n",
        "        # print('Pred: {0}'.format(pred_trg))\n",
        "        # print('Trg:  {0}'.format(trg_seq))\n",
        "        print(sentence_bleu([pred_trg.split(' ')], trg_seq.split(' ')))\n",
        "        I += 1\n",
        "        if I > 10:\n",
        "          break\n",
        "\n",
        "    print([pred_trgs])\n",
        "    print(trgs)\n",
        "    # return bleu_score(pred_trgs, trgs)\n",
        "    return sentence_bleu(pred_trgs, trgs)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "8f1585bb-c930-4fe1-d131-fbc2a1b6edaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "[['a a b a b c c c c c b a c b <eos>', 'c a a b <eos>', 'c b c <eos>', 'b c c a a c b a c a c c a c <eos>', 'c c a a a a a a b c a <eos>', 'c b c c a b c a a b b c b <eos>', 'a c b b b c a b c <eos>', 'a b a c <eos>', 'c b a a a <eos>', 'c c c c a a b a <eos>', 'a a b b a b c b b <eos>']]\n",
            "['a a b a b c c c c c b a c b <eos>', 'c a a b <eos>', 'c b c <eos>', 'b c c a a c b a c a c c a c <eos>', 'c c a a a a a a b c a <eos>', 'c b c c a b c a a b b c b <eos>', 'a c b b b c a b c <eos>', 'a b a c <eos>', 'c b a a a <eos>', 'c c c c a a b a <eos>', 'a a b b a b c b b <eos>']\n",
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTXEkYbWr2Xp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}