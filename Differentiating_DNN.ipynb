{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCnDNKE9qtiEXjMJa0NQHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "5d99c633-818e-4c78-bbb9-0be9e4cf98db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3",
        "outputId": "ad6b9b2d-78a5-463d-98a4-46f7794a8fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = 'vasiliyeskin'\n",
        "# user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "repo_name = 'differentiating_deep_neural_network'\n",
        "destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "### run first time if repo is absence\n",
        "# cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "### run next times\n",
        "cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "print(cmd_string)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Password: ··········\n",
            "git -C '/content/gdrive/My Drive/differentiating_deep_neural_network' pull\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "f7466b65-9377-453e-ae8d-e573d60b8a0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "5cd216e8-ea10-45df-8de0-df29c1fc0656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "bdde198f-ffce-4707-9229-a1e6e58b3686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "49613298-60e3-4597-dd4e-694c9b0b351b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "5b0d2144-c6e3-4b86-e4e2-244caddb3c5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "3d558205-5ee9-4cfc-f56c-445940f32a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "4bfd2ca0-57c3-4416-f3e4-aaab52e551a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0282\n",
            "Epoch 1 Batch 100 Loss 0.0095\n",
            "Epoch 1 Loss 0.0316\n",
            "Time taken for 1 epoch 11.756311655044556 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0349\n",
            "Epoch 2 Batch 100 Loss 0.0010\n",
            "Epoch 2 Loss 0.0036\n",
            "Time taken for 1 epoch 11.985961198806763 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0006\n",
            "Epoch 3 Batch 100 Loss 0.0003\n",
            "Epoch 3 Loss 0.0004\n",
            "Time taken for 1 epoch 11.567126512527466 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0002\n",
            "Epoch 4 Batch 100 Loss 0.0002\n",
            "Epoch 4 Loss 0.0002\n",
            "Time taken for 1 epoch 11.658431053161621 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0001\n",
            "Epoch 5 Batch 100 Loss 0.0001\n",
            "Epoch 5 Loss 0.0001\n",
            "Time taken for 1 epoch 11.39150094985962 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0001\n",
            "Epoch 6 Batch 100 Loss 0.0001\n",
            "Epoch 6 Loss 0.0001\n",
            "Time taken for 1 epoch 11.62341022491455 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0001\n",
            "Epoch 7 Batch 100 Loss 0.0001\n",
            "Epoch 7 Loss 0.0001\n",
            "Time taken for 1 epoch 11.427411317825317 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0001\n",
            "Epoch 8 Batch 100 Loss 0.0001\n",
            "Epoch 8 Loss 0.0001\n",
            "Time taken for 1 epoch 11.755339622497559 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0000\n",
            "Epoch 9 Batch 100 Loss 0.0001\n",
            "Epoch 9 Loss 0.0001\n",
            "Time taken for 1 epoch 11.524372100830078 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0000\n",
            "Epoch 10 Batch 100 Loss 0.0000\n",
            "Epoch 10 Loss 0.0000\n",
            "Time taken for 1 epoch 11.736368179321289 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "7f5f7e55-1569-41c0-cba4-a0080f965a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1028d59c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "8540e651-5b87-4c27-aa29-679576b22c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c b b b a <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "156c74a2-03c5-499d-94e2-73e643509602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b c')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c <eos>\n",
            "Predicted translation: c b a <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}