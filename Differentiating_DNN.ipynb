{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDp6XeWLKZ+ncCul1aNxG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    # forward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # backward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # self.gru = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,\n",
        "    #                      input_shape=(embedding_dim, self.dec_units))\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "03c42450-2748-4a98-8327-c682bdc35495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3"
      },
      "source": [
        "# import os\n",
        "# from getpass import getpass\n",
        "# import urllib\n",
        "\n",
        "# user = 'vasiliyeskin'\n",
        "# # user = input('User name: ')\n",
        "# password = getpass('Password: ')\n",
        "# password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# # repo_name = input('Repo name: ')\n",
        "# repo_name = 'differentiating_deep_neural_network'\n",
        "# destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "# ### run first time if repo is absence\n",
        "# # cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "# ### run next times\n",
        "# cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "# print(cmd_string)\n",
        "# os.system(cmd_string)\n",
        "# cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "fb821234-b44d-4657-c7de-96e7d48ddd78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "d8b75a98-003e-4e29-f258-961b4bbaa2b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "7754d6ba-d1b0-48c2-d9e5-380fdeba823c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "7dc6882d-4ab1-4d5a-f55d-8a49841b9497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "f81a128c-3fc5-41a7-dbb2-af2388e7a682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "eaeb8930-91fb-4e1f-8821-e41c8a43a884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "f0600221-d5ec-4f40-e32a-5bad89d3db69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3pXQDBUvjrt"
      },
      "source": [
        "@tf.function\n",
        "def validation_step(src, trg, enc_hidden, size):\n",
        "  loss = 0\n",
        "\n",
        "  enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  dec_src = tf.expand_dims([src_lang.word_index['<sos>']] * size, 1)\n",
        "\n",
        "  for t in range(1, max_length_src):\n",
        "    # passing enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "    loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "    dec_src = tf.expand_dims(predictions, 1)\n",
        "    \n",
        "    print(dec_src)\n",
        "    if  dec_src == trg_lang.word_index['<sos>']:\n",
        "      break\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "  return batch_loss"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "a324f456-fbd7-4184-ede1-a8f706d94686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "  # val_loss = validation_step(src, trg, enc_hidden, 64)\n",
        "  # print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1, val_loss))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.3362\n",
            "Epoch 1 Batch 100 Loss 0.2670\n",
            "Epoch 1 Loss 0.4034\n",
            "Time taken for 1 epoch 11.535084247589111 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2974\n",
            "Epoch 2 Batch 100 Loss 0.2954\n",
            "Epoch 2 Loss 0.3659\n",
            "Time taken for 1 epoch 11.895724773406982 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-ad831b63f564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id]\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "b6c449b7-0bef-4bdc-f597-04d0a7ea3c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f893fce5518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "75bb65ed-7e6f-44c8-e725-28218fa641e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c c c c c c c c c c c c \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "273d82c5-d51c-4a6f-def1-8b45595442f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c c c c c c c c c c c c \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj",
        "outputId": "f49b89a3-c8cc-4cbd-a6c0-8e989fcf3a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install torchtext with BLUE module\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     || 6.9MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NKpT4w0Dbu"
      },
      "source": [
        "def tensor_to_sequence(input):\n",
        "  result = ''\n",
        "  for id in range(1, input.shape[0]):\n",
        "    in_id = input[id].numpy()\n",
        "    result += trg_lang.index_word[in_id]\n",
        "\n",
        "    if trg_lang.index_word[in_id] == '<eos>':\n",
        "      return result\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "  return result[:-1]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "    I = 0\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data.shuffle(BATCH_SIZE)):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "\n",
        "        #cut off <eos> token\n",
        "        src_tensor = tf.reshape(src,[1, src.shape[0]])\n",
        "        #cut off <eos> token\n",
        "        # src_tensor = src_tensor[:-2]\n",
        "        # print(src_tensor)\n",
        "        pred_trg, _ = evaluate_of_tensor(src_tensor, attention_plot)\n",
        "        # print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        # pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trg_seq = tensor_to_sequence(trg)\n",
        "        trgs.append(trg_seq)\n",
        "        # print('Pred: {0}'.format(pred_trg))\n",
        "        # print('Trg:  {0}'.format(trg_seq))\n",
        "        print(sentence_bleu([pred_trg.split(' ')], trg_seq.split(' ')))\n",
        "        I += 1\n",
        "        if I > 10:\n",
        "          break\n",
        "\n",
        "    print([pred_trgs])\n",
        "    print(trgs)\n",
        "    # return bleu_score(pred_trgs, trgs)\n",
        "    return sentence_bleu(pred_trgs, trgs)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "43b0d185-34bb-47d4-9913-701faf6d53b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.08124013474973685\n",
            "0.07214533282472635\n",
            "0.024774512716690587\n",
            "0.14733461804689552\n",
            "0.2872477301817216\n",
            "0.23419920485587317\n",
            "0.05101445407132442\n",
            "0.19745987822060074\n",
            "0.28433113184914127\n",
            "0.2540542852726497\n",
            "0.24148526666444137\n",
            "[['c b c a c a c a c a c a c a c a ', 'c b c a c a c a c a c a c a c a ', 'c c c c c c c c c c c c c c c c ', 'c c b c a c a c a c a c a c a c ', 'b a c a c a c a c a c a c a c a ', 'b b b b b b b b b b b b b b b b ', 'c c c c c c c c c c c c c c c c ', 'a a a a a a a a a a a a a a a a ', 'b c c c c c c c c c c c c c c c ', 'b a c a c a c a c a c a c a c a ', 'b b b b b b b b b b b b b b b b ']]\n",
            "['c b c b a <eos>', 'c a b c <eos>', 'c c b <eos>', 'c b c c b a <eos>', 'b a c b c a a a c c b c <eos>', 'b b b a b a b a b a c c a b <eos>', 'c c c a <eos>', 'a a a c b a b a a <eos>', 'b c c b b a a c c c c c <eos>', 'b a a c c c a <eos>', 'b b b b b b c a <eos>']\n",
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}