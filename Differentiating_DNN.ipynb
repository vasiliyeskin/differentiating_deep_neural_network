{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv\n",
        "import random"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeK-BFaZm2UH"
      },
      "source": [
        "teacher_forcing_ratio = 0.75"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    # forward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # backward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # self.gru = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,\n",
        "    #                      input_shape=(embedding_dim, self.dec_units))\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "57353811-a401-4d27-9e1a-feeb709ce44c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3"
      },
      "source": [
        "# import os\n",
        "# from getpass import getpass\n",
        "# import urllib\n",
        "\n",
        "# user = 'vasiliyeskin'\n",
        "# # user = input('User name: ')\n",
        "# password = getpass('Password: ')\n",
        "# password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# # repo_name = input('Repo name: ')\n",
        "# repo_name = 'differentiating_deep_neural_network'\n",
        "# destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "# ### run first time if repo is absence\n",
        "# # cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "# ### run next times\n",
        "# cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "# print(cmd_string)\n",
        "# os.system(cmd_string)\n",
        "# cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "d4fdd272-0bb6-4b5d-f9ee-fbce26bc84de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "bd621e8e-60df-417d-9441-722c4e19c142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.1)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9000 9000 1000 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "be06658c-f0ca-4583-b214-9685bc6e4a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n",
        "# dataset_val = dataset_val.batch(BATCH_SIZE)\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "115a8869-9389-49d2-ed3c-58cf14d22514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "942c6031-c1ab-485d-cbc2-3e3b29d21bd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "2665b15d-04d7-47c9-ba69-f7ea4dc0ab4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "bf2507b7-f459-43fc-8280-26642ff42436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden, teacher_forcing_ratio = 0.5):\n",
        "  loss = 0\n",
        "\n",
        "  #decide if we are going to use teacher forcing or not\n",
        "  teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "\n",
        "      #if teacher forcing, use actual next token as next input\n",
        "      #if not, use predicted token\n",
        "      if teacher_force:\n",
        "        dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "      else:\n",
        "        pred = tf.argmax(predictions,1)\n",
        "        dec_src = tf.expand_dims(pred,1)\n",
        "\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3pXQDBUvjrt"
      },
      "source": [
        "def validation_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "  \n",
        "  print(src.shape)\n",
        "  print(enc_hidden.shape)\n",
        "\n",
        "  enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "  \n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "  for t in range(1, trg.shape[1]):\n",
        "    # passing enc_output to the decoder\n",
        "    predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "    loss += loss_function(trg[:, t], predictions)\n",
        "    \n",
        "    preds = tf.argmax(predictions,1)\n",
        "    dec_src = tf.expand_dims(preds, 1)\n",
        "    \n",
        "    # print(dec_src)\n",
        "    # if  dec_src == trg_lang.word_index['<sos>']:\n",
        "    #   break\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "  return batch_loss"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "1a892b2b-302b-41b2-fd80-a99d8b657db5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "min_loss = 100\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if min_loss > total_loss:\n",
        "    min_loss = total_loss\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "\n",
        "  # # enc_hidden = encoder.initialize_hidden_state()\n",
        "  # total_val_loss = 0\n",
        "  # for (batch, (src, trg)) in enumerate(dataset_val.take(steps_per_epoch)):\n",
        "  #   val_loss = validation_step(src, trg, enc_hidden)\n",
        "  #   total_val_loss += val_loss\n",
        "  # print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1, val_loss))\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5652\n",
            "Epoch 1 Batch 100 Loss 0.4812\n",
            "Epoch 1 Loss 0.4180\n",
            "Time taken for 1 epoch 13.136518239974976 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4593\n",
            "Epoch 2 Batch 100 Loss 0.2266\n",
            "Epoch 2 Loss 0.2762\n",
            "Time taken for 1 epoch 13.115626096725464 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2605\n",
            "Epoch 3 Batch 100 Loss 0.1165\n",
            "Epoch 3 Loss 0.1655\n",
            "Time taken for 1 epoch 13.094808578491211 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0476\n",
            "Epoch 4 Batch 100 Loss 0.0656\n",
            "Epoch 4 Loss 0.1064\n",
            "Time taken for 1 epoch 13.158676385879517 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0776\n",
            "Epoch 5 Batch 100 Loss 0.0372\n",
            "Epoch 5 Loss 0.1047\n",
            "Time taken for 1 epoch 13.203696012496948 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0362\n",
            "Epoch 6 Batch 100 Loss 0.0091\n",
            "Epoch 6 Loss 0.0145\n",
            "Time taken for 1 epoch 13.279189348220825 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0070\n",
            "Epoch 7 Batch 100 Loss 0.0038\n",
            "Epoch 7 Loss 0.0051\n",
            "Time taken for 1 epoch 13.229849815368652 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0045\n",
            "Epoch 8 Batch 100 Loss 0.0024\n",
            "Epoch 8 Loss 0.0030\n",
            "Time taken for 1 epoch 13.22653341293335 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0027\n",
            "Epoch 9 Batch 100 Loss 0.0018\n",
            "Epoch 9 Loss 0.0021\n",
            "Time taken for 1 epoch 13.224607706069946 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0021\n",
            "Epoch 10 Batch 100 Loss 0.0017\n",
            "Epoch 10 Loss 0.0016\n",
            "Time taken for 1 epoch 13.252564191818237 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id]\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "b09af93f-8708-41a3-bde6-fdedc1c0bc99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f206fc41320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "b1364e74-2d6f-431b-897c-7657afe5689b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c b b b a <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "a2c1cebe-25b4-4f37-cb0c-73fbe39b848f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c b a <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAI4CAYAAAABL2+hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcjklEQVR4nO3de7RtZ1nf8d+THEgMMWpiAAmgwwtCxGAIihpBB0ECirSilA4opB3i0ajUXKw2ahRLIRFEjaVFji3VcPHCxSI6FEmgBjFAEwREok1aFY6RcDFAQDjk8vaPtZJsNicnB3r2evez9uczxh7sPdfcZz8vZyfrmznnmqvGGAEAoKfDZg8AAMDnTswBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAA7kRVHVtVP1NV95w9y2ZibsWq6i5V9fCqOnL2LADAQXtykp9N8q8nz/EZxNzq/fMkb0jyL2cPAgActKcmecvyf7cVMbd6ZyT5h2zDsgcAPlNVnZjkpCRPSnLfqvr6ySN9GjG3QlV19yTfnkXVn1pVXzp5JADgzp2R5LVjjL9N8rvZZgdkxNxqPTnJ28cYr8/iVOu2O1QLANyuqg7L4vn74uWmlyR5YlXdZd5Un07MrdYZSV68/PwlSZ4ycRYA4M49MslRSX5v+fXrknwqyWOnTbSJmFuRqjopyQOS/OZy0yuT3KuqvmneVADAnXhqkleMMT6VJGOMW7J4Lj9j6lQbiLnVOSPJH40xPpQkY4yPJ/kf2Wbn3QGAhar6/CTfndtPsd7qJUm+o6q+ePVTfSYxtwJVdXgWr4B58aaHXpLkCVV119VPBQDcicOSPGaM8acbN44x/jzJI5LcPGWqTWqMMXuGtVdVX5Lk+5NceOth2uX2w5L8ZJKLxxjvmTUfANCXmAMAuANVdXySjDE+sPz6a5M8MclfjjF+80DfuypOs05SVZ9XVY90rzkA2NZ+J8l3JcnyGrnLsriO7ler6tyZg91KzK1IVf16Vf3Q8vO7Jnlrkj9O8tdV9ZipwwEAd+SkJG9efv69Sa4ZY3xNFq9y/YFpU20g5lbn9Nz+y/C4JJ+f5J5JnrH8AAC2n89L8rHl54/M7febe1uS+0yZaBMxtzpflOT9y88fneSVY4z3J/mtJCdOmwoAOJCrkzy+qu6T5FFZnFVLknsk+fC0qTYQc6vzviQPXN6m5PQklyy3H53kxmlTAQAH8nNJfj7J3yZ58xjjLcvtpyf581lDbbRr9gA7yIuS/HaSa7O4L82ly+0PTfJXs4YCAO7YGONVVXXfJPdK8o4ND12Sxbs5TefWJCtUVd+T5L5JXj7G2LvcdkaSD48xXj11OADggKrq6CRj+S5O24aYAwA4gKr64SQ/keSE5aa9SX5+jPFf5k11O6dZV6iqTkryY1m84GEkeXeS544x3jV1MABgv6rqJ5Ocl+QXktz6tl4PS3JhVR0zxrhw2nBLjsytSFU9Lsmrkrwxt/8yfMvy4/FjjNfMmg0A2L+qek+Sn9j8bg9V9eQkzx5jTL/5v5hbkap6Z5LfHWP87Kbt/yHJPxtjPGjOZADAHamqTyZ54Bjjmk3bvyrJX4wxjpwz2e3cmmR17pfkxfvZ/uIkX73iWQCAg/O/kzxpP9uflOSvVzzLfrlmbnXen+SUJNds2n5KkutWPw4AcBCekeR3qurhSd603HZqkm9N8oRZQ20k5lbn15K8sKq+MsmfLbedmsULIp47bSoA4A4t7zP30CRnJ3nscvNVSb5hjLEtbhrsmrkVqapKclaSc7O48WCyuIHwc5P8yvAXAQB8DsTcBFX1+Ukyxrhh9iwAwIFV1T2SPCXJlyf5mTHGB6vq1CTXjjH+Zu50XgCxMlV1WFUdltwWcXerqqdV1TdPHg0AuANVdUoWL3R4cpKnJTlm+dC3J3nWrLk2EnOr8wdJnp7c9nYgV2RxivVPquqpMwcDAO7QLyS5aIxxcpJ9G7a/Notr36cTc6vzkCSvX37++CQfTXL3JN+fxYsgAIDt55Qkv7Gf7f+Q5B4rnmW/xNzqHJ3kw8vPH5XFDYRvzCLwvmLaVADAgXwiyRftZ/v9s7jt2HRibnXek+TUqrpbktOTvG65/dgk/zRtKgDgQF6d5Ger6ojl16OqvizJzyd55ayhNhJzq/OLWbzbw94kf5/ksuX2hyf5i1lDAQAH9GNZHHj5QJKjsnh/9WuSfCTJT0+c6zZuTbJCy1fE3DfJ68YYH1tu+84kHx5jvOmA3wwATFNVj0jy4CwOhL1tjHHJ5JFuI+ZWoKq+IMlJY4w37uexU5O8e4xx/eonAwDuSJfnb6dZV+OWJH+4/Iu/TVU9KIsXQBw+ZSoA4EBaPH97b9YVGGPcUFWvTvLU3P4mvcnibtKvHWN8cM5kW6+qdiX5hixOL99142NjjIunDLVCy3sK5tbT6gD00eX525G51bk4yROq6q7J4h0hkjwpya/PHGorVdX9s3gz4suSvDTJf81ivb+W5PnzJtt6VXVWVb0niwtkP1JV762qs5fv0buWqupZVfWD+9n+g1X1zBkzrYJ1f8Z2615DO3XdS9v++VvMrc7rsrhXzWOXX5+WxZGq10ybaOv9cpIrk3xBFrdfeUAWN09+e5LvmTjXlqqq5yR5RpIXZvF2L9+e5FeT/EwWL2VfV09J8uf72X5lFv9Vu66s+9NZ93raqetOGjx/O826ImOMW6rqJVn80r8qi38wfnt54+B19fVJvnWM8fGquiXJrjHG26rqx5P8pyQnzR1vyzwtydPGGK/YsO31VfXXWQTej88Za8vdPYuX7m/2oWyTu6RvEev+dNa9nnbquls8fzsyt1oXJ3l0Vd03yXdn/28Psk4qt98Q+QNJTlh+vjfJV06ZaHXeeQfb1vmfufckedh+tj88i7/zdWXdn86619NOXfettvXztyNzKzTG+MuqelcW14/tHWO8dfZMW+xdSR6U5P8meWuSn6iqm7N4P9prZg62xS5O8sNJfnTT9jOzuHH0unphkl9aXldy6/sQn5bkgqz36WXrtm7rXnPb/flbzK3exVlcS/ZTswdZgWcludvy859O8gdJ3pDkg0n+xayhtkJV/cqGL3cl+VdVdXqSNy+3PTTJvbL4F8FaGmM8r6q+OMmv5PZXLn8qyUVjjOfMm2xrWbd1x7rXdt2bbNvnbzcNXrGqOjbJ05O8cIzxvtnzrNpy/dePNfvFq6o3HOSuY4zxiC0dZrLl+w+fuPzyqp1yWxbrTmLda2+nrjvZ3s/fYg4AoLF1vhgbAGDtibkJqmr37BlmsO6dxbp3FuveWax7exFzc2zLX4YVsO6dxbp3FuveWax7GxFzAACN7dgXQNy1jhhH3nbXjNW6Mftylxwx5WfPZN07i3VPMPGdf28c+3KXmrTuiU9jM/++v+qkj0/5uUnygQ/dnOOPO3zKz776nXOeu5O5f9835PoPjjGO399jO/Y+c0fmbnlonTZ7DIBDpnbtzH+lj5tumj3CFL//R1fOHmGKx55wyuwRprhkvOLv7ugxp1kBABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNrEXO1cG5VXV1V+6pqb1VdMHsuAICttmv2AIfIs5OcmeScJJclOT7JyZt3qqrdSXYnyZE5apXzAQBsifYxV1VHJzk7yVljjBctN1+T5PLN+44x9iTZkyTH1LFjZUMCAGyRdTjNemKSI5JcOnsQAIBVW4eYAwDYsdYh5q5Ksi/JabMHAQBYtfbXzI0xbqiqi5JcUFX7sngBxHFJThljvGDudAAAW6t9zC2dl+T6JOcnuXeS65JcPHUiAIAVWIuYG2PckuTC5QcAwI6xDtfMAQDsWGIOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGts1ewAADo1x002zR2CF7lKHzx6BbcKROQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANLYWMVcL51bV1VW1r6r2VtUFs+cCANhqu2YPcIg8O8mZSc5JclmS45OcvHmnqtqdZHeSHJmjVjkfAMCWaB9zVXV0krOTnDXGeNFy8zVJLt+87xhjT5I9SXJMHTtWNiQAwBZZh9OsJyY5IsmlswcBAFi1dYg5AIAdax1i7qok+5KcNnsQAIBVa3/N3Bjjhqq6KMkFVbUvixdAHJfklDHGC+ZOBwCwtdrH3NJ5Sa5Pcn6Seye5LsnFUycCAFiBtYi5McYtSS5cfgAA7BjrcM0cAMCOJeYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgsV2zBwDg0Hjv+d88e4Qp7vPMP5s9whRf/sffN3uEKb4qV84eYdtxZA4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2tRczVwrlVdXVV7auqvVV1wey5AAC22q7ZAxwiz05yZpJzklyW5PgkJ2/eqap2J9mdJEfmqFXOBwCwJdrHXFUdneTsJGeNMV603HxNkss37zvG2JNkT5IcU8eOlQ0JALBF1uE064lJjkhy6exBAABWbR1iDgBgx1qHmLsqyb4kp80eBABg1dpfMzfGuKGqLkpyQVXty+IFEMclOWWM8YK50wEAbK32Mbd0XpLrk5yf5N5Jrkty8dSJAABWYC1iboxxS5ILlx8AADvGOlwzBwCwY4k5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaGzX7AGArVO7duY/4uOmm2aPMMXLv+95s0eY4pxnftPsEaa4/7l/M3uEKW6ePcA25MgcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaW4uYq4Vzq+rqqtpXVXur6oLZcwEAbLVdswc4RJ6d5Mwk5yS5LMnxSU7evFNV7U6yO0mOzFGrnA8AYEu0j7mqOjrJ2UnOGmO8aLn5miSXb953jLEnyZ4kOaaOHSsbEgBgi6zDadYTkxyR5NLZgwAArNo6xBwAwI61DjF3VZJ9SU6bPQgAwKq1v2ZujHFDVV2U5IKq2pfFCyCOS3LKGOMFc6cDANha7WNu6bwk1yc5P8m9k1yX5OKpEwEArMBaxNwY45YkFy4/AAB2jHW4Zg4AYMcScwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANDYrtkDAFvnn77zwbNHmOLzXv3W2SNMccG1j5k9wiQfnj3AFLd85KOzR2CbcGQOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNrUXM1cK5VXV1Ve2rqr1VdcHsuQAAttqu2QMcIs9OcmaSc5JcluT4JCdv3qmqdifZnSRH5qhVzgcAsCXax1xVHZ3k7CRnjTFetNx8TZLLN+87xtiTZE+SHFPHjpUNCQCwRdbhNOuJSY5IcunsQQAAVm0dYg4AYMdah5i7Ksm+JKfNHgQAYNXaXzM3xrihqi5KckFV7cviBRDHJTlljPGCudMBAGyt9jG3dF6S65Ocn+TeSa5LcvHUiQAAVmAtYm6McUuSC5cfAAA7xjpcMwcAsGOJOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANLZr9gDA1nnvo2dPMMf9Xj17gjmuuOQBs0eY4ktz+ewR5ijHY1jwmwAA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBorH3MVdX/rKrnz54DAGCG9jEHALCTiTkAgMbWJeZ2VdVFVXX98uO5VbUuawMAuEPrEjxPzmIt35TkB5LsTnLW5p2qandVXVFVV9yYfSseEQDg0Ns1e4BD5B+S/NsxxkjyV1V1vyTnJPnFjTuNMfYk2ZMkx9SxY+VTAgAcYutyZO7Ny5C71eVJTqiqY2YNBACwCusScwAAO9K6xNxDq6o2fP2NSa4dY3x01kAAAKuwLjF3ryS/XFVfXVXfm+TfJfmlyTMBAGy5dXkBxEuTHJ7kLUlGkv8WMQcA7ADtY26M8W0bvvyRWXMAAMywLqdZAQB2JDEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGds0eANg63/mQd8weYYqrZw8wyb0u+9TsEYAJHJkDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxtrHXFU9uqreWFXXV9U/VtVrq+oBs+cCAFiF9jGX5G5JfjnJNyT5tiQfSfKaqrrr5h2randVXVFVV9yYfaudEgBgC+yaPcD/rzHGKzd+XVX/JslHs4i7P920754ke5LkmDp2rGpGAICt0v7IXFV9RVW9rKr+T1V9NMl1WazrvpNHAwDYcu2PzCX5/SR7k/xAkr9PclOSdyf5jNOsAADrpnXMVdVxSe6f5IfGGG9Ybntwmq8LAOBgdY+e65N8MMn3V9V7k5yQ5LlZHJ0DAFh7ra+ZG2PckuSJSU5K8q4k/znJ+YmXqgIAO0P3I3MZY7w+yQM3bT56xiwAAKvW+sgcAMBOJ+YAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANCYmAMAaEzMAQA0JuYAABoTcwAAjYk5AIDGxBwAQGNiDgCgMTEHANDYrtkDAFvn+Se8ZfYIU5yer5s9whR3ueTK2SOwQuPGT80egW3CkTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0JiYAwBoTMwBADQm5gAAGhNzAACNiTkAgMbEHABAY2IOAKAxMQcA0NghibmqOqaqvvBQ/FkH8bO+sKqOWcXPAgDY7j7nmKuqw6vq9Kp6WZL3JXnQcvsXVNWeqnp/Vd1QVX9SVQ/Z9L2Pr6q/qKp9VfXeqvqpqqpNj7+zqj5RVf+4/DPusXz4QUneV1UvXf78wz/XNQAAdPdZx1xVfU1VPSfJe5P8dpKPJ3l0ksuWQfYHSU5I8tgkJye5LMnrq+pLlt9/SpKXJ3lVkq9N8u+TnJfkR5aP3zPJbyX5jSQPSPLwJC/eMMJly5/3ieXPf09VPaeqvuYgZt9dVVdU1RU3Zt9nu3QAgG2nxhh3vlPVcUmenOSMLALsj7IIrNeMMT65Yb9HJPm9JMePMT6xYfvbk7xsjPGcqnppki8ZYzxiw+PPSPK0Mca9q+rBSa5M8mVjjL+7k7mOTPK4JE9NcnqSdyS5OMlLxxgfOtD3HlPHjofWaXe6dujstde+ffYIU5x+r6+bPQLAIXXJeMWVY4yH7O+xgz0y9/QkFyX5ZJL7jTEeN8Z4+caQWzolyVFJPlBVH7v1I8kDk3zFcp8HJHnTpu/70yQnLK+Fe0eSS5K8q6peWVVnVtXx+xtqjPHJMcbvjDEem+R+SW5czvn0g1wXAEBruw5yvz1ZhNJTs4is383iyNylY4ybN+x3WJLrkjxsP3/GRw/i54wxxs1V9agk35jkUUm+L8kFVfWtY4x3bNx5eb3cI5M8Jcl3Z3Hq96eT/PeDXBcAQGsHdWRujHHtGONZY4yvziKePpbFdW17q+p5VXXrOY23JblHklvGGNds+nj/cp+rkpy66Ud8S5K9Y4wblj9vjDEuH2P8XJKvT3JtkifeunNVnVxVv5hkb5LfTHJDktPGGPdfznntZ/9/BQBAPwd7ZO42Y4w3J3lzVZ2V5LuyuI7ufy2vl7ski1Oor66qH0/yV0numcULFi4ZY7wxyfOW+z8jycuyiLVzk/xkklTVN2YRjK/N4ijfyUnuk+Tdy8cfluT1Sf4wi9OprxljeDUDALAjfdYxd6tlQL0iySuq6u5Jbh5jjKr6jiT/McmvJbl7FkH2pixemJAxxtuq6glJfi6LgLsuyYVJnr/8oz+SxZG7pyf5wixOnT5zjPGS5ePvTnLChiN9AAA71ucccxttDKvlqdIfXX7c0f6vyuLWJPt77KokjznA9x7wVaoAADuJt/MCAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoDExBwDQmJgDAGhMzAEANCbmAAAaE3MAAI2JOQCAxsQcAEBjYg4AoLEaY8yeYYqq+kCSv5v04784yQcn/eyZrHtnse6dxbp3FutevS8dYxy/vwd2bMzNVFVXjDEeMnuOVbPuncW6dxbr3lmse3txmhUAoDExBwDQmJibY8/sASax7p3FuncW695ZrHsbcc0cAEBjjswBADQm5gAAGhNzAACNiTkAgMbEHABAY/8PRcyL3CQNp4QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj"
      },
      "source": [
        "# # install torchtext with BLUE module\n",
        "# !pip3 install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NKpT4w0Dbu"
      },
      "source": [
        "def tensor_to_sequence(input):\n",
        "  result = ''\n",
        "  for id in range(1, input.shape[0]):\n",
        "    in_id = input[id].numpy()\n",
        "    result += trg_lang.index_word[in_id]\n",
        "\n",
        "    if trg_lang.index_word[in_id] == '<eos>':\n",
        "      return result\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "  return result[:-1]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "    I = 0\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data.shuffle(BATCH_SIZE)):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "\n",
        "        #cut off <eos> token\n",
        "        src_tensor = tf.reshape(src,[1, src.shape[0]])\n",
        "        #cut off <eos> token\n",
        "        # src_tensor = src_tensor[:-2]\n",
        "        # print(src_tensor)\n",
        "        pred_trg, _ = evaluate_of_tensor(src_tensor, attention_plot)\n",
        "        # print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        # pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trg_seq = tensor_to_sequence(trg)\n",
        "        trgs.append(trg_seq)\n",
        "        # print('Pred: {0}'.format(pred_trg))\n",
        "        # print('Trg:  {0}'.format(trg_seq))\n",
        "        bleu_score = sentence_bleu([pred_trg.split(' ')], trg_seq.split(' '))\n",
        "        print(f'BLEU score = {bleu_score*100:.2f}')\n",
        "        I += 1\n",
        "        if I > 100:\n",
        "          break\n",
        "\n",
        "    # print([pred_trgs])\n",
        "    # print(trgs)\n",
        "    # return bleu_score(pred_trgs, trgs)\n",
        "    return sentence_bleu(pred_trgs, trgs)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "b30bb1cf-e882-47dc-9e81-77d3c5968eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 100.00\n",
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTXEkYbWr2Xp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}