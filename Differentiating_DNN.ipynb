{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwpb4T1ndwk7U5QD0cFwk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "ec1c3957-8f89-4063-a17f-a8d1b9eb9f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3",
        "outputId": "ad6b9b2d-78a5-463d-98a4-46f7794a8fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = 'vasiliyeskin'\n",
        "# user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "repo_name = 'differentiating_deep_neural_network'\n",
        "destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "### run first time if repo is absence\n",
        "# cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "### run next times\n",
        "cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "print(cmd_string)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Password: ··········\n",
            "git -C '/content/gdrive/My Drive/differentiating_deep_neural_network' pull\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "bd2be61c-62be-4346-b648-541463a7a4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "b86e8c36-90e9-4a29-bd3b-19f3202cbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "de7be5c0-85cf-4d8e-f69f-e1a17a88d28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "edfd22ac-f1a4-4347-a834-9197f9b0c733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "f048c587-b8f2-41c3-f1dd-01298c2ca31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "80a336fd-7b20-4569-d4c8-c235f0a0f19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "2ef6e750-6d60-494a-beee-d2afef3589ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "b5ff5dc0-2de3-465f-a039-834bc0fe362d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1050\n",
            "Epoch 1 Batch 100 Loss 0.5480\n",
            "Epoch 1 Loss 0.6971\n",
            "Time taken for 1 epoch 28.143463850021362 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5330\n",
            "Epoch 2 Batch 100 Loss 0.5850\n",
            "Epoch 2 Loss 0.5862\n",
            "Time taken for 1 epoch 11.381104230880737 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4848\n",
            "Epoch 3 Batch 100 Loss 0.5116\n",
            "Epoch 3 Loss 0.5227\n",
            "Time taken for 1 epoch 11.019082307815552 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5318\n",
            "Epoch 4 Batch 100 Loss 0.3834\n",
            "Epoch 4 Loss 0.4050\n",
            "Time taken for 1 epoch 11.500761270523071 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2147\n",
            "Epoch 5 Batch 100 Loss 0.2067\n",
            "Epoch 5 Loss 0.2262\n",
            "Time taken for 1 epoch 11.20753526687622 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0541\n",
            "Epoch 6 Batch 100 Loss 0.0787\n",
            "Epoch 6 Loss 0.0783\n",
            "Time taken for 1 epoch 11.695879936218262 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0140\n",
            "Epoch 7 Batch 100 Loss 0.0380\n",
            "Epoch 7 Loss 0.0340\n",
            "Time taken for 1 epoch 11.421447992324829 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0095\n",
            "Epoch 8 Batch 100 Loss 0.0089\n",
            "Epoch 8 Loss 0.0557\n",
            "Time taken for 1 epoch 11.892610788345337 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0048\n",
            "Epoch 9 Batch 100 Loss 0.0088\n",
            "Epoch 9 Loss 0.0055\n",
            "Time taken for 1 epoch 11.661799192428589 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0030\n",
            "Epoch 10 Batch 100 Loss 0.1531\n",
            "Epoch 10 Loss 0.2473\n",
            "Time taken for 1 epoch 12.140012264251709 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "e8fc5d3d-0466-45c9-932e-10fcf932df0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4b4c94f748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "7af1b1ec-1566-453d-f581-975076fb51de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c c b b b a <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "e93b7702-70b8-41c8-870b-4613764a5adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c c b a <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj"
      },
      "source": [
        "# install torchtext with BLUE module\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NKpT4w0Dbu"
      },
      "source": [
        "def tensor_to_sequence(input):\n",
        "  result = ''\n",
        "  for id in range(1, input.shape[0]):\n",
        "    in_id = input[id].numpy()\n",
        "    result += trg_lang.index_word[in_id]\n",
        "\n",
        "    if trg_lang.index_word[in_id] == '<eos>':\n",
        "      return result\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "  return result[:-1]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "        pred_trg, _ = evaluate_of_tensor(tf.reshape(src,[1, src.shape[0]]), attention_plot)\n",
        "        print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append(tensor_to_sequence(trg))\n",
        "        # print(pred_trgs)\n",
        "        # print(trgs)\n",
        "        # break\n",
        "\n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "cb156ffb-89c8-43af-db2b-88378602b065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a a c b b c b a a a a b <eos> \n",
            "a c b <eos> \n",
            "c b a b a b <eos> \n",
            "b c c b c b b b <eos> \n",
            "b a a c a c a c a c a b a <eos> \n",
            "b a c a a a c c c c <eos> \n",
            "b a a b a a a a b a c b b <eos> \n",
            "c a b b b c c c a b b c <eos> \n",
            "a b a <eos> \n",
            "a c c c <eos> \n",
            "b a b a a c b b c a b a <eos> \n",
            "c b a b <eos> \n",
            "c b c b b a c b c b b <eos> \n",
            "c b b c b c c a a a <eos> \n",
            "a c a b a b a b b b b c <eos> \n",
            "c a a c b a <eos> \n",
            "c c a a a a a b c <eos> \n",
            "c a c <eos> \n",
            "a c c a c c a c a c a b c b b b \n",
            "c c b a b b <eos> \n",
            "a c b a a c b c c <eos> \n",
            "b c a c c a <eos> \n",
            "c b b a b <eos> \n",
            "b b c c b b a c c <eos> \n",
            "a a c b a a a c <eos> \n",
            "a b b c c c c b b b c c a <eos> \n",
            "a a b c c b b <eos> \n",
            "a c c b c c c b c c c c c a <eos> \n",
            "c b b b a a b a a a c <eos> \n",
            "c c c a b b c a b b a b c <eos> \n",
            "a a b b b <eos> \n",
            "a b a b a a <eos> \n",
            "c b b b c c b b <eos> \n",
            "b b c b b c a b b <eos> \n",
            "b b c c c c c a b a b c b <eos> \n",
            "b a a a c b c <eos> \n",
            "a a a c b c c c b c b b a <eos> \n",
            "c a a a a a b c c c a a <eos> \n",
            "c a a c c a c <eos> \n",
            "c a c a c a c a c <eos> \n",
            "c c c <eos> \n",
            "b b a a b a b b a c a b <eos> \n",
            "a b b c c a b a a c <eos> \n",
            "a a c c a c c a <eos> \n",
            "c b a c c a a b c a <eos> \n",
            "a c a c c c a a c b a c b a c a \n",
            "b b b b c <eos> \n",
            "b b a c <eos> \n",
            "b b a c c b <eos> \n",
            "c a a c a a c b c b c c c <eos> \n",
            "a c a c c b a a b c <eos> \n",
            "b c a a c b <eos> \n",
            "a b a b c c c b b <eos> \n",
            "a a a b c a b a <eos> \n",
            "c c a a <eos> \n",
            "a b b c c b c b a c a a b b <eos> \n",
            "a c a b a c a b a a <eos> \n",
            "c b b <eos> \n",
            "c b b a a b c a b <eos> \n",
            "a a c a b c <eos> \n",
            "c a a c a a c a <eos> \n",
            "c a b b c b b a <eos> \n",
            "b a a <eos> \n",
            "b b c b b a a a c <eos> \n",
            "c a a c c c a c c a b a a c <eos> \n",
            "c b c c b a b a a c <eos> \n",
            "a c a b c a b b c b b c c <eos> \n",
            "c b a b c b a a a <eos> \n",
            "b a c c a a a a b a c <eos> \n",
            "c b c c a <eos> \n",
            "b c c a c a c a c a a a c a <eos> \n",
            "b b c c a a c c a c c a b a b c \n",
            "a b b a c c a b a <eos> \n",
            "a a a a c a b b c a b b <eos> \n",
            "a b a c c c b c <eos> \n",
            "a a b b a a a c a b a a <eos> \n",
            "c b c a b a a a b a b a c <eos> \n",
            "c b b b a c <eos> \n",
            "c c b c b c a c c b a c a a <eos> \n",
            "a a b <eos> \n",
            "a c c b b c c a <eos> \n",
            "a b c a b c a a c c c b b <eos> \n",
            "a c c b b b a c a a a c a <eos> \n",
            "a c a b b b a b a <eos> \n",
            "a b a b c a c c <eos> \n",
            "a c a b b c a b a a c b b a <eos> \n",
            "c b c c b c a b c b <eos> \n",
            "a c a b b a b c b a a <eos> \n",
            "b c a a c b b a <eos> \n",
            "a b a c a c <eos> \n",
            "b b a c c a c a c a c b c c a a \n",
            "a c a b a <eos> \n",
            "b b c c b b a c a c <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-be5b8d602463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'BLEU score = {bleu_score*100:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-174-2418f6851afc>\u001b[0m in \u001b[0;36mcalculate_bleu\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# pred_trg, _ = translate(src)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print(tf.reshape(src[1:],[1, src.shape[0]-1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpred_trg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_of_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_trg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-b12c2991443a>\u001b[0m in \u001b[0;36mevaluate_of_tensor\u001b[0;34m(input_tensors, attention_plot)\u001b[0m\n\u001b[1;32m     11\u001b[0m     predictions, dec_hidden, attention_weights = decoder(dec_input,\n\u001b[1;32m     12\u001b[0m                                                          \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                          enc_out)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# storing the attention weights to plot later on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-94f1de57530a>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# x shape after passing through embedding == (batch_size, 1, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup_v2\u001b[0;34m(params, ids, max_norm, name)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0;31m# params. Similar to the case np > 1 where parallel_dynamic_stitch is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;31m# outside the scioe of all with ops.colocate_with(params[p]).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;31m# Flatten the ids. There are two cases where we need to do this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3987\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3988\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Identity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3989\u001b[0;31m         tld.op_callbacks, input)\n\u001b[0m\u001b[1;32m   3990\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3991\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nujd6bFSifds"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}