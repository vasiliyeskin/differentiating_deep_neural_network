{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYJxz1/zP+uzrqj1Jf1Kbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    # forward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # backward_layer = tf.keras.layers.GRU(self.dec_units,\n",
        "    #                                return_sequences=True,\n",
        "    #                                return_state=True,\n",
        "    #                                recurrent_initializer='glorot_uniform')\n",
        "    # self.gru = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,\n",
        "    #                      input_shape=(embedding_dim, self.dec_units))\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "22be751a-576f-45c2-b004-d1b835e66244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3"
      },
      "source": [
        "# import os\n",
        "# from getpass import getpass\n",
        "# import urllib\n",
        "\n",
        "# user = 'vasiliyeskin'\n",
        "# # user = input('User name: ')\n",
        "# password = getpass('Password: ')\n",
        "# password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# # repo_name = input('Repo name: ')\n",
        "# repo_name = 'differentiating_deep_neural_network'\n",
        "# destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "# ### run first time if repo is absence\n",
        "# # cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "# ### run next times\n",
        "# cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "# print(cmd_string)\n",
        "# os.system(cmd_string)\n",
        "# cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "3c50ee25-be92-4c41-e5f2-2d9cd0417fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "613150c0-0eaf-432c-85c8-f8b801aa1a2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "c0cf6312-c787-493f-d2e3-1c5b68baf032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "1 ----> c\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "1 ----> c\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n"
      ],
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "1a5bc5d3-e5c4-476e-8334-435cf237772c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "be903689-32ff-4e6f-ed78-2681500463c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "d6bb6576-52d7-4cdb-acab-351bbd7cc373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "dac15140-eaec-42c2-f404-ca536f5aebb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "ee4f3d95-aae0-47aa-cf63-835e5afef87f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.0538\n",
            "Epoch 1 Batch 100 Loss 0.6781\n",
            "Epoch 1 Loss 0.6981\n",
            "Time taken for 1 epoch 27.563319206237793 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6310\n",
            "Epoch 2 Batch 100 Loss 0.4778\n",
            "Epoch 2 Loss 0.5753\n",
            "Time taken for 1 epoch 11.510440111160278 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4600\n",
            "Epoch 3 Batch 100 Loss 0.5577\n",
            "Epoch 3 Loss 0.4564\n",
            "Time taken for 1 epoch 10.999593257904053 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3234\n",
            "Epoch 4 Batch 100 Loss 0.2065\n",
            "Epoch 4 Loss 0.2818\n",
            "Time taken for 1 epoch 11.44457197189331 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1814\n",
            "Epoch 5 Batch 100 Loss 0.2575\n",
            "Epoch 5 Loss 0.2388\n",
            "Time taken for 1 epoch 11.145206212997437 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1067\n",
            "Epoch 6 Batch 100 Loss 0.0285\n",
            "Epoch 6 Loss 0.0922\n",
            "Time taken for 1 epoch 11.56676697731018 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0098\n",
            "Epoch 7 Batch 100 Loss 0.4648\n",
            "Epoch 7 Loss 0.1330\n",
            "Time taken for 1 epoch 11.252132177352905 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1529\n",
            "Epoch 8 Batch 100 Loss 0.0880\n",
            "Epoch 8 Loss 0.2516\n",
            "Time taken for 1 epoch 11.655194759368896 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4611\n",
            "Epoch 9 Batch 100 Loss 0.4566\n",
            "Epoch 9 Loss 0.3331\n",
            "Time taken for 1 epoch 11.351048707962036 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3944\n",
            "Epoch 10 Batch 100 Loss 0.6874\n",
            "Epoch 10 Loss 0.4479\n",
            "Time taken for 1 epoch 11.762847900390625 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id]\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "f5c7588a-e910-47eb-dec8-8e7e5b56b454",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4b24a78668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "cf46c338-346e-4dae-9b5c-189158548012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c b c c b c c b c c b c c b \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "bd1d5f9c-37da-4b0b-9582-7fe42d1419aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c c c c c c c c c c c c \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAJoCAYAAAAJTgjfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbCElEQVR4nO3dfYxd+VnY8e/jeHaM10nVOBNoYkIFNKEu3dR4QtRuG6o4YYG2tGyLqIhg+0c6xUVt/RIRGWpCX/AYglqZSnFjJAuGrSg0BNKqKtvEQTWlJhEKNASc4FFLtlPA2U2H4qRwk3qe/nHuxi8d9p6HuWfubzzfj3S1nuNjn2fs1df3nN+950ZmIknqb8+sB5CkncZwSlKR4ZSkIsMpSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZT0QImIF0fEd0fEFwx1jCbDGRFzEfG6iNg361kk7ThvAt4G/K2hDtBkOIG/Bvws8DdnPYikHedbgQ+M/zuIVsP5BPBbDPgvhqQHT0QcBh4Bvhl4RUS8ZojjNBfOiHgp8Ea6fy0ejYgvmvFIknaOJ4CnMvM3gJ9ioCdfzYWT7vrEL2fm++lO1wd7ui3pwRERe+j6sTLe9CTwTRExN+1jtRjOJ4AfHf/4SeBbZjiLpJ3jDcB+4N+Ov34v8BngL0/7QE2FMyIeAf4k8GPjTT8JvCwi/uzsppK0Q3wr8K7M/AxAZm7QteSJaR+oqXDSfYM/k5mfBMjMTwM/jYtEkp5HRLwQ+AbunKY/50ng6yLiJdM8XjPhjIgX0K2E/eh9P/Uk8I0R8dD2TyVph9gDfG1m/ue7N2bmLwGvB25P82DRyoe1RcQfA/42cP65p9rj7XuA7wRWMvPpWc0nSc9pJpyStBURsQCQmc+Mv/7TwDcBv5qZP/Z8v7aqmVP1zUTE50XEG3wtp6QefgL4KwDja5pX6a57/suIOD3NAzUVzoj44Yj4u+MfPwR8EPiPwMci4mtnOpyk1j0C/ML4x38DWM3MP0W32v53pnmgpsIJPMadb/zrgRcCXwB8z/ghSX+QzwM+Nf7xG7jzes4PAV84zQO1Fs4/Cnxi/OOvAX4yMz8B/Gvg8MymkrQT3AAej4gvBL6a7mwV4POB35nmgVoL528DXz5+adJjwPvG2w8An53ZVJJ2gn8EfB/wG8AvZOYHxtsfA35pmgfaO83fbAouAz8O/Cbd666ujLe/FvjorIaS1L7MfHdEvAJ4GfBf7/qp99G9C3Fqmns5UkT8deAVwL/JzLXxtieA38nM98x0OEk7QkQcAHL87sPp//6thVOS/rAi4tuBtwIvH29aA74vM98xzeO0dqr+3I0+3kK3GJTArwFvz8yPzHQwSU2LiO8EzgA/ADz31su/AJyPiBdl5vmpHaulZ5wR8fXAu4Gf4843/ufHj8cz89/NajZJbYuIp4G33v8uoYh4E3AuM6f2RprWwvlh4Kcy8233bf/HwF/NzFfPZjJJrYuI3we+PDNX79v+J4Bfycypffhjay9HeiX//92RGG971TbPImln+XW6O6zd75uBj03zQK1d4/wEcBRYvW/7UeDm9o8jaQf5HuAnIuJ1wM+Ptz0KfBXwjdM8UGvh/CHgnRHxpcB/GW97lG6x6O0zm0pS88av43wtcJI7H5dxHfjK8X05p6a1a5wBnABO072IFboXw78d+MFsaVhJu1ZT4bzb+Fb4ZOatWc8iaWeIiM+n+4DHLwa+OzOfjYhHgd/MzP8+reM0tTgUEXvGd3x/LpgPR8SbI+LPzXg0SY2LiKN0i0BvAt4MvGj8U28Evneax2oqnMC/B/4efO4tU79Id5r+nyLCz1eX9Hx+ALiQmUeA0V3bn6JbK5ma1sK5CLx//OPHgd8FXkr3WURvmdVQknaEo8CPbLL9t+huLTc1rYXzAHfum/fVdC+G/yxdTL9kZlNJ2gl+j+6evvf7Mu7c53cqWgvn08CjEfEw3T303jve/mLg/8xsKkk7wXuAt0XE/PjrjIg/TnePzqneVq61cP4zuncJrQH/k+7DlgBeB/zKrIaStCO8he5J1jPAfrr7XawC/xv4h9M8UHMvRxqvjL0CeG9mfmq87S/R3Y/z55/3F0va9SLi9cBX0D0x/FBmvm/CL6kfo5VwRsQfAR7JzJ/b5OceBX4tM9e3fzJJrdvufrR0qr4B/IfxN/k5EfFqusWhF8xkKkk7wbb2o5n3qmfmrYh4D91nIN99Sv4twFOZ+ex2zhMRe4GvpLts8NDdP5eZK9s5y3ieA+Njf2rSvtJus+39yMxmHnQr6f8LeGj89R6696o/vs1zfBndR43+X7oPjfsM3b9oI+B3t3mWE3SvNrg9fvwPupsYxDYd/3uBb9tk+7cB/2Qb/xycwzkmzbJt/di2b6rnN76HbjX98fHXbwSeBea2eY6fofss94eBW3SvIf0K4APAG7dxju+ne13rdwGvHz++C1gHvn+bZngaeO0m218DfHwb/yycwzkmzbJt/WjmVB0gMzci4km6p9vvpnua/ePZvQh+O70G+KrM/HREbAB7M/NDEfEdwL8AHtmmOd4MvDkz33XXtvdHxMeAdwLfsQ0zvJTu5R33+yRTfjeGczjHVmxnP1paHHrOCvA1489H/gY2fwvV0II7L7h/hns/Me9Lt3mWD/8B27br7+5pug+8ut/r6P48totzOEcf29KPpp5xAmTmr0bER4B/Baxl5gdnMMZHgFcD/w34IPDWiLhN9575++9OP6QV4NuBf3Df9uNs/hEjQ3gn8M8j4iHu3EfgGLBM946M7eIczjHRtvVjO69BFK5V/H26xZgzMzr+Y9y5TvLFdHeR3qB7v+tfHPjYP3jX4x10Nzr5KPDD48d1undCvGMb/zyW6d4H/NwC1e8B52fw9+IcztFnnsH70cwL4O8WES+mu73cOzPzt2c9D3xupvUc+A8sIn62566Zma8fcpa7je8fcHj85fWc0cuinMM5eswyeD+aDKcktazFxSFJaprhlKSipsMZEUuzngGc437OcS/nuNdumKPpcAJN/AXgHPdzjns5x70e+DlaD6ckNWewVfWHYj738fCWfo/PMmKO+ck7Dsw5nGO3zPHKR7b+CTXPfPI2Cwe3dhe3X//w/i3PMY0/j1usP5uZC/dvH+ydQ/t4mNfGsaF+e0kDeOqpX571CAA89rI/M+sRAHhfvuvjm233VF2SigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JamoVzijczoibkTEKCLWImJ56OEkqUV9byt3DjgOnAKuAgvAkaGGkqSWTQxnRBwATgInMvPyePMqcG2TfZcY365+H1u/EakktajPqfphYB64MmnHzLyUmYuZudjCHbElaQguDklSUZ9wXgdGgJ+DIUn0uMaZmbci4gKwHBEjusWhg8DRzLw49ICS1Jq+q+pngHXgLHAIuAmsDDWUJLWsVzgzcwM4P35I0q7m4pAkFRlOSSoynJJUZDglqchwSlKR4ZSkIsMpSUWGU5KKDKckFfV9y6X0YIqY9QQAvOCVXzLrEQD46U+vznqEHcFnnJJUZDglqchwSlKR4ZSkIsMpSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZSkIsMpSUWGU5KKDKckFfUKZ3ROR8SNiBhFxFpELA89nCS1qO+NjM8Bx4FTwFVgATgy1FCS1LKJ4YyIA8BJ4ERmXh5vXgWubbLvErAEsI/9UxxTktrR51T9MDAPXJm0Y2ZeyszFzFycY37Lw0lSi1wckqSiPuG8DoyAYwPPIkk7wsRrnJl5KyIuAMsRMaJbHDoIHM3Mi0MPKEmt6buqfgZYB84Ch4CbwMpQQ0lSy3qFMzM3gPPjhyTtai4OSVKR4ZSkIsMpSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlJR37dc6gGx9+Uvm/UIAPz+j7Txv17805fMegQA9nzwo7MeAYCLr3rVrEcYuz3rAZ6XzzglqchwSlKR4ZSkIsMpSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZSkIsMpSUWGU5KKDKckFRlOSSrqFc7onI6IGxExioi1iFgeejhJalHfu8meA44Dp4CrwAJwZKihJKllE8MZEQeAk8CJzLw83rwKXNtk3yVgCWAf+6c4piS1o8+p+mFgHrgyacfMvJSZi5m5OMf8loeTpBa5OCRJRX3CeR0YAccGnkWSdoSJ1zgz81ZEXACWI2JEtzh0EDiamReHHlCSWtN3Vf0MsA6cBQ4BN4GVoYaSpJb1CmdmbgDnxw9J2tVcHJKkIsMpSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZSkor5vudQD4vYnnp31CAB89vbLZz0CAPO3c9YjALDnRS+c9QgAxHwbt4O8vb4+6xGel884JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkq6hXO6JyOiBsRMYqItYhYHno4SWpR3xsZnwOOA6eAq8ACcGSooSSpZRPDGREHgJPAicy8PN68ClzbZN8lYAlgH/unOKYktaPPqfphYB64MmnHzLyUmYuZuThHG7fgl6Rpc3FIkor6hPM6MAKODTyLJO0IE69xZuatiLgALEfEiG5x6CBwNDMvDj2gJLWm76r6GWAdOAscAm4CK0MNJUkt6xXOzNwAzo8fkrSruTgkSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZSkIsMpSUV933KpB8WemPUEAMy94PasR+hkznoCALKROciNWU+wI/iMU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZSkIsMpSUWGU5KKDKckFRlOSSoynJJUZDglqchwSlKR4ZSkol7hjM7piLgREaOIWIuI5aGHk6QW9b2R8TngOHAKuAosAEeGGkqSWjYxnBFxADgJnMjMy+PNq8C1TfZdApYA9rF/imNKUjv6nKofBuaBK5N2zMxLmbmYmYtzzG95OElqkYtDklTUJ5zXgRFwbOBZJGlHmHiNMzNvRcQFYDkiRnSLQweBo5l5cegBJak1fVfVzwDrwFngEHATWBlqKElqWa9wZuYGcH78kKRdzcUhSSoynJJUZDglqchwSlKR4ZSkIsMpSUWGU5KKDKckFRlOSSrq+5ZLaao2MmY9AuAzB/3h+P+NJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFvcIZndMRcSMiRhGxFhHLQw8nSS3qeyPjc8Bx4BRwFVgAjgw1lCS1bGI4I+IAcBI4kZmXx5tXgWub7LsELAHsY/8Ux5SkdvQ5VT8MzANXJu2YmZcyczEzF+eY3/JwktQiF4ckqahPOK8DI+DYwLNI0o4w8RpnZt6KiAvAckSM6BaHDgJHM/Pi0ANKUmv6rqqfAdaBs8Ah4CawMtRQktSyXuHMzA3g/PghSbuai0OSVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVNQrnNE5HRE3ImIUEWsRsTz0cJLUor099zsHHAdOAVeBBeDIUENJUssmhjMiDgAngROZeXm8eRW4tsm+S8ASwD72T3FMSWpHn1P1w8A8cGXSjpl5KTMXM3NxjvktDydJLXJxSJKK+oTzOjACjg08iyTtCBOvcWbmrYi4ACxHxIhuceggcDQzLw49oCS1pu+q+hlgHTgLHAJuAitDDSVJLesVzszcAM6PH5K0q7k4JElFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFvcIZndMRcSMiRhGxFhHLQw8nSS3a23O/c8Bx4BRwFVgAjgw1lCS1bGI4I+IAcBI4kZmXx5tXgWub7LsELAHsY/8Ux5SkdvQ5VT8MzANXJu2YmZcyczEzF+eY3/JwktQiF4ckqahPOK8DI+DYwLNI0o4w8RpnZt6KiAvAckSM6BaHDgJHM/Pi0ANKUmv6rqqfAdaBs8Ah4CawMtRQktSyXuHMzA3g/PghSbuai0OSVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkq6hXO6JyOiBsRMYqItYhYHno4SWrR3p77nQOOA6eAq8ACcGSooSSpZRPDGREHgJPAicy8PN68ClzbZN8lYAlgH/unOKYktaPPqfphYB64MmnHzLyUmYuZuTjH/JaHk6QWuTgkSUV9wnkdGAHHBp5FknaEidc4M/NWRFwAliNiRLc4dBA4mpkXhx5QklrTd1X9DLAOnAUOATeBlaGGkqSW9QpnZm4A58cPSdrVXBySpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpKJe4YzO6Yi4ERGjiFiLiOWhh5OkFu3tud854DhwCrgKLABHhhpKklo2MZwRcQA4CZzIzMvjzavAtU32XQKWAPaxf4pjSlI7+pyqHwbmgSuTdszMS5m5mJmLc8xveThJapGLQ5JU1Cec14ERcGzgWSRpR5h4jTMzb0XEBWA5IkZ0i0MHgaOZeXHoASWpNX1X1c8A68BZ4BBwE1gZaihJalmvcGbmBnB+/JCkXc3FIUkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkq6hXO6JyOiBsRMYqItYhYHno4SWrR3p77nQOOA6eAq8ACcGSooSSpZRPDGREHgJPAicy8PN68ClzbZN8lYAlgH/unOKYktaPPqfphYB64MmnHzLyUmYuZuTjH/JaHk6QWuTgkSUV9wnkdGAHHBp5FknaEidc4M/NWRFwAliNiRLc4dBA4mpkXhx5QklrTd1X9DLAOnAUOATeBlaGGkqSW9QpnZm4A58cPSdrVXBySpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpKJe4YzO6Yi4ERGjiFiLiOWhh5OkFu3tud854DhwCrgKLABHhhpKklo2MZwRcQA4CZzIzMvjzavAtU32XQKWAPaxf4pjSlI7+pyqHwbmgSuTdszMS5m5mJmLc8xveThJapGLQ5JU1Cec14ERcGzgWSRpR5h4jTMzb0XEBWA5IkZ0i0MHgaOZeXHoASWpNX1X1c8A68BZ4BBwE1gZaihJalmvcGbmBnB+/JCkXc3FIUkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkq6hXO6JyOiBsRMYqItYhYHno4SWrR3p77nQOOA6eAq8ACcGSooSSpZRPDGREHgJPAicy8PN68ClzbZN8lYAlgH/unOKYktaPPqfphYB64MmnHzLyUmYuZuTjH/JaHk6QWuTgkSUV9wnkdGAHHBp5FknaEidc4M/NWRFwAliNiRLc4dBA4mpkXhx5QklrTd1X9DLAOnAUOATeBlaGGkqSW9QpnZm4A58cPSdrVXBySpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpKJe4YzO6Yi4ERGjiFiLiOWhh5OkFu3tud854DhwCrgKLABHhhpKklo2MZwRcQA4CZzIzMvjzavAtU32XQKWAPaxf4pjSlI7+pyqHwbmgSuTdszMS5m5mJmLc8xveThJapGLQ5JU1Cec14ERcGzgWSRpR5h4jTMzb0XEBWA5IkZ0i0MHgaOZeXHoASWpNX1X1c8A68BZ4BBwE1gZaihJalmvcGbmBnB+/JCkXc3FIUkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkq6hXO6JyOiBsRMYqItYhYHno4SWrR3p77nQOOA6eAq8ACcGSooSSpZRPDGREHgJPAicy8PN68ClzbZN8lYAlgH/unOKYktaPPqfphYB64MmnHzLyUmYuZuTjH/JaHk6QWuTgkSUV9wnkdGAHHBp5FknaEidc4M/NWRFwAliNiRLc4dBA4mpkXhx5QklrTd1X9DLAOnAUOATeBlaGGkqSW9QpnZm4A58cPSdrVXBySpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVGQ4JanIcEpSkeGUpKJe4YzO6Yi4ERGjiFiLiOWhh5OkFu3tud854DhwCrgKLABHhhpKklo2MZwRcQA4CZzIzMvjzavAtU32XQKWAPaxf4pjSlI7+pyqHwbmgSuTdszMS5m5mJmLc8xveThJapGLQ5JU1Cec14ERcGzgWSRpR5h4jTMzb0XEBWA5IkZ0i0MHgaOZeXHoASWpNX1X1c8A68BZ4BBwE1gZaihJalmvcGbmBnB+/JCkXc3FIUkqMpySVGQ4JanIcEpSkeGUpCLDKUlFhlOSigynJBUZTkkqMpySVBSZOcxvHPEM8PEt/jYvAZ6dwjhb5Rz3co57Oce9HqQ5vigzF+7fOFg4pyEifjEzF53DOZzDOVqaw1N1SSoynJJU1Ho4L816gDHnuJdz3Ms57vXAz9H0NU5JalHrzzglqTmGU5KKDKckFRlOSSoynJJU9P8AvtTlhs48KugAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj",
        "outputId": "7c1815d3-ad12-4f96-b26f-dc4db9c9b350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install torchtext with BLUE module\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.8.0 in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NKpT4w0Dbu"
      },
      "source": [
        "def tensor_to_sequence(input):\n",
        "  result = ''\n",
        "  for id in range(1, input.shape[0]):\n",
        "    in_id = input[id].numpy()\n",
        "    result += trg_lang.index_word[in_id]\n",
        "\n",
        "    if trg_lang.index_word[in_id] == '<eos>':\n",
        "      return result\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "  return result[:-1]"
      ],
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "    I = 0\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data.shuffle(BATCH_SIZE)):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "\n",
        "        #cut off <eos> token\n",
        "        src_tensor = tf.reshape(src,[1, src.shape[0]])\n",
        "        #cut off <eos> token\n",
        "        # src_tensor = src_tensor[:-2]\n",
        "        # print(src_tensor)\n",
        "        pred_trg, _ = evaluate_of_tensor(src_tensor, attention_plot)\n",
        "        # print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        # pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trg_seq = tensor_to_sequence(trg)\n",
        "        trgs.append(trg_seq)\n",
        "        # print('Pred: {0}'.format(pred_trg))\n",
        "        # print('Trg:  {0}'.format(trg_seq))\n",
        "        print(sentence_bleu([pred_trg.split(' ')], trg_seq.split(' ')))\n",
        "        I += 1\n",
        "        if I > 10:\n",
        "          break\n",
        "\n",
        "    print([pred_trgs])\n",
        "    print(trgs)\n",
        "    # return bleu_score(pred_trgs, trgs)\n",
        "    return sentence_bleu(pred_trgs, trgs)"
      ],
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "a434bfa7-b9d8-4646-cc76-995e52b78b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.13484470220547534\n",
            "0.09661134626816685\n",
            "0.24181521090264727\n",
            "0.21457092748493922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.03260509236999754\n",
            "0.30822348736495186\n",
            "0.2540542852726497\n",
            "0.5814307369682193\n",
            "0.14436497957551314\n",
            "0.237461765940911\n",
            "0.2457484553230998\n",
            "[['a a a a a a a a a a a a a a a a ', 'a a b b b b b b b b b b b b b b ', 'c c a a c a a c a a c a a c a a ', 'b b c b b b b b b b b b b b b b ', 'a a a a a a a a a a a a a a a a ', 'c b a a b a a b a a b a a b a a ', 'c c c c c c c c c c c c c c c c ', 'b a c <eos>', 'c a a a a a a a a a a a a a a a ', 'b b c b c b c b c b c b c b c b ', 'b b a b a b a b a b a b a b a b ']]\n",
            "['a a a b a c a <eos>', 'a c b a b <eos>', 'c a b b c c c a a c <eos>', 'b b b b a c c a a a b b b <eos>', 'a b a <eos>', 'c b a a b b b c a b c a a a <eos>', 'c b a c a a c <eos>', 'b a c a a c <eos>', 'c a b a c a a a b <eos>', 'b b c a c a c b c b a c b <eos>', 'b a b b c b a c a a <eos>']\n",
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}