{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differentiating_DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7g6VHCa9xV5ZduG501F7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Zj9NzJ3LaM"
      },
      "source": [
        "# Differentiating Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiEnxax3ygf"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/Differentiating_DNN.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GGpvYck8rky"
      },
      "source": [
        "Used model Seq2Seq-with-attention is based on the model which is written in https://www.tensorflow.org/tutorials/text/nmt_with_attention and https://github.com/tensorflow/nmt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRG3-hnyVXX"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkx_jQPn_zp0"
      },
      "source": [
        "## The encoder and decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWY2R9LBAFem"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwar-Cp6AWT7"
      },
      "source": [
        "Implement of [Bahdanau Attention](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhQpHa4Ak0P"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyNGx-8AtOu"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUuqnm1Ax3P"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_VpNyMEBr0r",
        "outputId": "ec1c3957-8f89-4063-a17f-a8d1b9eb9f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !wget https://github.com/vasiliyeskin/differentiating_deep_neural_network/blob/main/toy_revert/train.csv\n",
        "\n",
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNnhlFupEgd3",
        "outputId": "ad6b9b2d-78a5-463d-98a4-46f7794a8fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = 'vasiliyeskin'\n",
        "# user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "# repo_name = input('Repo name: ')\n",
        "repo_name = 'differentiating_deep_neural_network'\n",
        "destination_dir = '/content/gdrive/My Drive/{0}'.format(repo_name)\n",
        "\n",
        "### run first time if repo is absence\n",
        "# cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}.git \\'{3}\\''.format(user, password, repo_name, destination_dir)\n",
        "\n",
        "### run next times\n",
        "cmd_string = 'git -C \\'{0}\\' pull'.format(destination_dir)\n",
        "\n",
        "print(cmd_string)\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Password: ··········\n",
            "git -C '/content/gdrive/My Drive/differentiating_deep_neural_network' pull\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6a3aJMRbr9j"
      },
      "source": [
        "## Test of the model on the revert sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8BIA84ZS3Nx"
      },
      "source": [
        "repo_dir = '/content/gdrive/My Drive/differentiating_deep_neural_network'\n",
        "path_to_file = repo_dir + \"/toy_revert/train.csv\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphxS1SUMcf"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bObsnB9XqmE",
        "outputId": "bd2be61c-62be-4346-b648-541463a7a4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return word pairs in the format: [src, inverse src]\n",
        "def create_dataset(path, num_examples):\n",
        "  # lines = io.open(path).read().strip().split('\\n')\n",
        "\n",
        "  # word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  word_pairs = [[preprocess_sentence(row['src']), preprocess_sentence(row['trg'])] for row in csv.DictReader(open(path, newline=''))]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "src, trg = create_dataset(path_to_file, None)\n",
        "print(src[-2])\n",
        "print(trg[-2])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> a a c a <eos>\n",
            "<sos> a c a a <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMEIyEEaZmld"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoianK2lZpam"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  src, trg = create_dataset(path, num_examples)\n",
        "\n",
        "  src_tensor, src_lang_tokenizer = tokenize(src)\n",
        "  trg_tensor, trg_lang_tokenizer = tokenize(trg)\n",
        "\n",
        "  return src_tensor, trg_tensor, src_lang_tokenizer, trg_lang_tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0AAFhVnaZy7",
        "outputId": "b86e8c36-90e9-4a29-bd3b-19f3202cbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "src_tensor, trg_tensor, src_lang, trg_lang = load_dataset(path_to_file)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_trg, max_length_src = trg_tensor.shape[1], src_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(src_tensor_train), len(trg_tensor_train), len(src_tensor_val), len(trg_tensor_val))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000 8000 2000 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJbgIaIcInH"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9hM53cOcLAl",
        "outputId": "de7be5c0-85cf-4d8e-f69f-e1a17a88d28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Input sequence; index to word mapping\")\n",
        "convert(src_lang, src_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target sequence; index to word mapping\")\n",
        "convert(trg_lang, trg_tensor_train[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "3 ----> a\n",
            "1 ----> c\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "5 ----> <eos>\n",
            "\n",
            "Target sequence; index to word mapping\n",
            "4 ----> <sos>\n",
            "2 ----> b\n",
            "3 ----> a\n",
            "2 ----> b\n",
            "1 ----> c\n",
            "3 ----> a\n",
            "5 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHZesSc_cfGZ"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90eFiNXych1M"
      },
      "source": [
        "BUFFER_SIZE = len(src_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(src_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(src_lang.word_index) + 1\n",
        "vocab_tar_size = len(trg_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_tensor_train, trg_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((src_tensor_val, trg_tensor_val))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjFTaSKtc2s1",
        "outputId": "edfd22ac-f1a4-4347-a834-9197f9b0c733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_src_batch, example_trg_batch = next(iter(dataset))\n",
        "example_src_batch.shape, example_trg_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGV5ukqmdHPK"
      },
      "source": [
        "### Get encode and decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2s48uZedMx7",
        "outputId": "f048c587-b8f2-41c3-f1dd-01298c2ca31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_src_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP_xxQYPecx8",
        "outputId": "80a336fd-7b20-4569-d4c8-c235f0a0f19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention_layer = Attention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8w8UDMkeqc4",
        "outputId": "2ef6e750-6d60-494a-beee-d2afef3589ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBtocxze6K4"
      },
      "source": [
        "### Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxssNvIiety3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k1nocs6fGdv"
      },
      "source": [
        "### Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c6rqGc3fI-0"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMkTwfxCfZ4E"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfTMVPbBfc_D"
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, trg, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(src, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_src = tf.expand_dims([trg_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next src\n",
        "    for t in range(1, trg.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_src, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(trg[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_src = tf.expand_dims(trg[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(trg.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3msflmffxk2",
        "outputId": "b5ff5dc0-2de3-465f-a039-834bc0fe362d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (src, trg)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(src, trg, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1050\n",
            "Epoch 1 Batch 100 Loss 0.5480\n",
            "Epoch 1 Loss 0.6971\n",
            "Time taken for 1 epoch 28.143463850021362 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5330\n",
            "Epoch 2 Batch 100 Loss 0.5850\n",
            "Epoch 2 Loss 0.5862\n",
            "Time taken for 1 epoch 11.381104230880737 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4848\n",
            "Epoch 3 Batch 100 Loss 0.5116\n",
            "Epoch 3 Loss 0.5227\n",
            "Time taken for 1 epoch 11.019082307815552 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5318\n",
            "Epoch 4 Batch 100 Loss 0.3834\n",
            "Epoch 4 Loss 0.4050\n",
            "Time taken for 1 epoch 11.500761270523071 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2147\n",
            "Epoch 5 Batch 100 Loss 0.2067\n",
            "Epoch 5 Loss 0.2262\n",
            "Time taken for 1 epoch 11.20753526687622 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0541\n",
            "Epoch 6 Batch 100 Loss 0.0787\n",
            "Epoch 6 Loss 0.0783\n",
            "Time taken for 1 epoch 11.695879936218262 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0140\n",
            "Epoch 7 Batch 100 Loss 0.0380\n",
            "Epoch 7 Loss 0.0340\n",
            "Time taken for 1 epoch 11.421447992324829 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0095\n",
            "Epoch 8 Batch 100 Loss 0.0089\n",
            "Epoch 8 Loss 0.0557\n",
            "Time taken for 1 epoch 11.892610788345337 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0048\n",
            "Epoch 9 Batch 100 Loss 0.0088\n",
            "Epoch 9 Loss 0.0055\n",
            "Time taken for 1 epoch 11.661799192428589 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0030\n",
            "Epoch 10 Batch 100 Loss 0.1531\n",
            "Epoch 10 Loss 0.2473\n",
            "Time taken for 1 epoch 12.140012264251709 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko_cujukvpa"
      },
      "source": [
        "### Revert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THH7bE6lMjY"
      },
      "source": [
        "def evaluate_of_tensor(input_tensors, attention_plot):\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(input_tensors, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_trg):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += trg_lang.index_word[predicted_id]\n",
        "\n",
        "    if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, attention_plot\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, attention_plot"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJFUS98kz2Y"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [src_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_src,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  # result = ''\n",
        "\n",
        "  # hidden = [tf.zeros((1, units))]\n",
        "  # enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  # dec_hidden = enc_hidden\n",
        "  # dec_input = tf.expand_dims([trg_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  # for t in range(max_length_trg):\n",
        "  #   predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "  #                                                        dec_hidden,\n",
        "  #                                                        enc_out)\n",
        "\n",
        "  #   # storing the attention weights to plot later on\n",
        "  #   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "  #   attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "  #   predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "  #   result += trg_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "  #   if trg_lang.index_word[predicted_id] == '<eos>':\n",
        "  #     return result, sentence, attention_plot\n",
        "\n",
        "  #   # the predicted ID is fed back into the model\n",
        "  #   dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  # return result, sentence, attention_plot\n",
        "  result, attention_plot = evaluate_of_tensor(inputs, attention_plot)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwROPNezlJ1v"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMiGkiiklg5E"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTmNo3silo4p"
      },
      "source": [
        "### Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dqun3kdlqnp",
        "outputId": "9d581200-f963-4518-a4bd-3cf890d14f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4aa27ade10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6gIRRJLlxOO",
        "outputId": "c249813c-cda4-43c9-8324-59388bf6d474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b b b c c c c c')"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b b b c c c c c <eos>\n",
            "Predicted translation: c c c c c c b b b a <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjFiTVunbVm",
        "outputId": "db667c34-6458-4cf7-8c08-9a92546409c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "source": [
        "translate('a b c c c c c')"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> a b c c c c c <eos>\n",
            "Predicted translation: c c c c c c b a <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyAz1l9Yldj"
      },
      "source": [
        "# install torchtext with BLUE module\n",
        "!pip3 install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0NKpT4w0Dbu"
      },
      "source": [
        "def tensor_to_sequence(input):\n",
        "  result = ''\n",
        "  for id in range(1, input.shape[0]):\n",
        "    in_id = input[id].numpy()\n",
        "    result += trg_lang.index_word[in_id]\n",
        "\n",
        "    if trg_lang.index_word[in_id] == '<eos>':\n",
        "      return result\n",
        "\n",
        "    result += ' '\n",
        "\n",
        "  return result[:-1]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpVLOQ2GoZe"
      },
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu(data):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    attention_plot = np.zeros((max_length_trg, max_length_src))\n",
        "\n",
        "    I = 0\n",
        "    \n",
        "    for _, (src, trg) in enumerate(data.shuffle(BATCH_SIZE)):\n",
        "        \n",
        "        # src = vars(datum)['src']\n",
        "        # trg = vars(datum)['trg']\n",
        "        \n",
        "        # pred_trg, _ = translate(src)\n",
        "        # print(tf.reshape(src[1:],[1, src.shape[0]-1]))\n",
        "\n",
        "        #cut off <eos> token\n",
        "        src_tensor = tf.reshape(src,[1, src.shape[0]])\n",
        "        #cut off <eos> token\n",
        "        # src_tensor = src_tensor[:-2]\n",
        "        # print(src_tensor)\n",
        "        pred_trg, _ = evaluate_of_tensor(src_tensor, attention_plot)\n",
        "        # print(pred_trg)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        # pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trg_seq = tensor_to_sequence(trg)\n",
        "        trgs.append(trg_seq)\n",
        "        # print('Pred: {0}'.format(pred_trg))\n",
        "        # print('Trg:  {0}'.format(trg_seq))\n",
        "        print(sentence_bleu([pred_trg.split(' ')], trg_seq.split(' ')))\n",
        "        I += 1\n",
        "        if I > 10:\n",
        "          break\n",
        "\n",
        "    print([pred_trgs])\n",
        "    print(trgs)\n",
        "    # return bleu_score(pred_trgs, trgs)\n",
        "    return sentence_bleu(pred_trgs, trgs)"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lie3WBWYFNI",
        "outputId": "fdb41148-736f-433d-ff3a-213679726d9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bleu_score = calculate_bleu(dataset_val)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.7420016786832134\n",
            "1.0\n",
            "1.0\n",
            "0.8005529884898329\n",
            "1.0\n",
            "0.729836014355472\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.9131007162822624\n",
            "[['c b c b b a c b c b b <eos>', 'a c a c c c a a c b a c b a c a ', 'b c c b c b b b <eos>', 'b a a a c b c <eos>', 'b a a b a a a a b a c b b <eos>', 'a c b <eos>', 'c a a c c c a c c a b a a c <eos>', 'a a c a b c <eos>', 'c b a b c b a a a <eos>', 'a b a <eos>', 'b a c c a a a a b a c <eos>']]\n",
            "['c b c b b a c b c b b <eos>', 'a c a c c c a a c b a c a <eos>', 'b c c b c b b b <eos>', 'b a a a c b c <eos>', 'b a b a a a b a c b b <eos>', 'a c b <eos>', 'c a a c c c a c b a a c <eos>', 'a a c a b c <eos>', 'c b a b c b a a a <eos>', 'a b a <eos>', 'b a c c a a a b a c <eos>']\n",
            "BLEU score = 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}